{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Gleaner Documentation \uf0c1 About \uf0c1 This directory holds documentation related to Gleaner. Much of this documentation is a bit dated with more current information being developed for the UN Ocean InfoHub . We will soon back port the material generated for that project into this repository for maintenance here.","title":"Home"},{"location":"#gleaner-documentation","text":"","title":"Gleaner Documentation"},{"location":"#about","text":"This directory holds documentation related to Gleaner. Much of this documentation is a bit dated with more current information being developed for the UN Ocean InfoHub . We will soon back port the material generated for that project into this repository for maintenance here.","title":"About"},{"location":"BinaryInstallation/","text":"Installing Release Binaries \uf0c1 Release binaries are automatically built when a release is created. Release builds do not always build, but they can be rerun with the hope that all will complete. https://github.com/gleanerio/gleaner/releases Two executables are built: * gleaner * glcon * Linux, Windows \uf0c1 download and uncompress ./gleaner ./glcon --help Mac \uf0c1 Using the executable build requires an extra step. download and uncompress. * run, a dialog will appear, click ok * open settings>security and privacy. at bottom of tab, it should ask that you approve the app * another dialog confirms this change Now you can run the executables ./gleaner ./glcon --help (if someone wants to build a signed mac executable... https://www.kencochrane.com/2020/08/01/build-and-sign-golang-binaries-for-macos-with-github-actions/) Directory Layout \uf0c1 The directory includes a directory for configuration files, the latest json-ld schema from schema.or and the executable.","title":"BinaryInstallation"},{"location":"BinaryInstallation/#installing-release-binaries","text":"Release binaries are automatically built when a release is created. Release builds do not always build, but they can be rerun with the hope that all will complete. https://github.com/gleanerio/gleaner/releases Two executables are built: * gleaner * glcon *","title":"Installing Release Binaries"},{"location":"BinaryInstallation/#linux-windows","text":"download and uncompress ./gleaner ./glcon --help","title":"Linux, Windows"},{"location":"BinaryInstallation/#mac","text":"Using the executable build requires an extra step. download and uncompress. * run, a dialog will appear, click ok * open settings>security and privacy. at bottom of tab, it should ask that you approve the app * another dialog confirms this change Now you can run the executables ./gleaner ./glcon --help (if someone wants to build a signed mac executable... https://www.kencochrane.com/2020/08/01/build-and-sign-golang-binaries-for-macos-with-github-actions/)","title":"Mac"},{"location":"BinaryInstallation/#directory-layout","text":"The directory includes a directory for configuration files, the latest json-ld schema from schema.or and the executable.","title":"Directory Layout"},{"location":"DEVELOPER/","text":"Developer notes \uf0c1 Repositories \uf0c1 Githubactions \uf0c1 glcon \uf0c1 glcon integrates gleaner and nabu to make a single env integrating with local nabu \uf0c1 see article `require ( .... github.com/gleanerio/nabu v0.0.0-20211214151422-eda9e525f196 ... } replace ( github.com/gleanerio/nabu v0.0.0-20211214151422-eda9e525f196 => ../nabu )` REMOVE WHEN COMMITTING FOR A PULL REQUEST update nabu dependency \uf0c1 if nabu code has been updated, the you need to update the dependency go get -u github.com/gleanerio/nabu","title":"Developer notes"},{"location":"DEVELOPER/#developer-notes","text":"","title":"Developer notes"},{"location":"DEVELOPER/#repositories","text":"","title":"Repositories"},{"location":"DEVELOPER/#githubactions","text":"","title":"Githubactions"},{"location":"DEVELOPER/#glcon","text":"glcon integrates gleaner and nabu to make a single env","title":"glcon"},{"location":"DEVELOPER/#integrating-with-local-nabu","text":"see article `require ( .... github.com/gleanerio/nabu v0.0.0-20211214151422-eda9e525f196 ... } replace ( github.com/gleanerio/nabu v0.0.0-20211214151422-eda9e525f196 => ../nabu )` REMOVE WHEN COMMITTING FOR A PULL REQUEST","title":"integrating with local nabu"},{"location":"DEVELOPER/#update-nabu-dependency","text":"if nabu code has been updated, the you need to update the dependency go get -u github.com/gleanerio/nabu","title":"update nabu dependency"},{"location":"GleanerConfig/","text":"Gleaner Configuration file \uf0c1 This assumes that you have a container stack running s3 store triple store headless Gleaner Configuration generation \uf0c1 Files can be generated using glcon. Described in README_CONFIGURE_Template Gleaner Configuration \uf0c1 When generated, the Gleaner configuration file named gleaner in the configs/{config} directory, but any name with a .yaml ending is acceptable. There is actually quite a bit in this file, but for this starting demo only a few things we need to worry about. The default file will look like: --- minio: address: 0.0.0.0 port: 9000 accessKey: worldsbestaccesskey secretKey: worldsbestsecretkey ssl: false bucket: gleaner gleaner: runid: runX # this will be the bucket the output is placed in... summon: true # do we want to visit the web sites and pull down the files mill: true context: cache: true contextmaps: - prefix: \"https://schema.org/\" file: \"./configs/schemaorg-current-https.jsonld\" - prefix: \"http://schema.org/\" file: \"./configs/schemaorg-current-https.jsonld\" summoner: after: \"\" # \"21 May 20 10:00 UTC\" mode: full # full || diff: If diff compare what we have currently in gleaner to sitemap, get only new, delete missing threads: 5 delay: # milliseconds (1000 = 1 second) to delay between calls (will FORCE threads to 1) headless: http://127.0.0.1:9222 # URL for headless see docs/headless millers: graph: true # will be built from sources.csv sources: - sourcetype: sitegraph name: aquadocs logo: \"\" url: https://oih.aquadocs.org/aquadocs.json headless: false pid: http://hdl.handle.net/1834/41372 propername: AquaDocs domain: https://aquadocs.org active: false - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: false A few things we need to look at. First, in the \"mino:\" section make sure the accessKey and secretKey here match the ones you have and set via your demo.env file. Next, lets look at the \"gleaner:\" section. We can set the runid to something. This is the ID for a run and it allows you to later make different runs and keep the resulting graphs organized. It can be set to any lower case string with no spaces. The miller and summon sections are true and we will leave them that way. It means we want Gleaner to both fetch the resources and process (mill) them. Now look at the \"miller:\" section when lets of pick what milling to do. Currently it is set with only graph set to true. Let's leave it that way for now. This means Gleaner will only attempt to make graph and not also run validation or generate prov reports for the process. The final section we need to look at is the \"sources:\" section. Here is where the fun is. While there are two types, sitegraph and sitemaps we will normally use sitemap type. A standard sitemap is below: sources: - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: true A sitegraph sources: - sourcetype: sitegraph name: aquadocs logo: \"\" url: https://oih.aquadocs.org/aquadocs.json headless: false pid: http://hdl.handle.net/1834/41372 propername: AquaDocs domain: https://aquadocs.org active: false A Google Drive sources: - sourcetype: googledrive name: ecrr_submitted logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd headless: false pid: \"\" propername: Earthcube Resource Registry domain: http://www.earthcube.org/resourceregistry/ active: true credentialsfile: configs/credentials/gleaner-331805-030e15e1d9c4.json other: {} These are the sources we wish to pull and process. Each source has a type, and 8 entries though at this time we no longer use the \"logo\" value. It was used in the past to provide a page showing all the sources and a logo for them. However, that's really just out of scope for what we want to do. You can leave it blank or set it to any value, it wont make a difference. The name is what you want to call this source. It should be one word (no space) and be lower case. The url value needs to point to the URL for the site map XML file. This will be created and served by the data provider. The headless value should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page. This is true of some sites and it is supported but not currently auto-detected. So you will need to know this and set it. For most place, this will be false. You can have as many sources as you wish. For an example look the configure file for the CDF Semantic Network at: https://github.com/gleanerio/CDFSemanticNetwork/blob/master/configs/cdf.yaml","title":"Gleaner Configuration file"},{"location":"GleanerConfig/#gleaner-configuration-file","text":"This assumes that you have a container stack running s3 store triple store headless","title":"Gleaner Configuration file"},{"location":"GleanerConfig/#gleaner-configuration-generation","text":"Files can be generated using glcon. Described in README_CONFIGURE_Template","title":"Gleaner Configuration generation"},{"location":"GleanerConfig/#gleaner-configuration","text":"When generated, the Gleaner configuration file named gleaner in the configs/{config} directory, but any name with a .yaml ending is acceptable. There is actually quite a bit in this file, but for this starting demo only a few things we need to worry about. The default file will look like: --- minio: address: 0.0.0.0 port: 9000 accessKey: worldsbestaccesskey secretKey: worldsbestsecretkey ssl: false bucket: gleaner gleaner: runid: runX # this will be the bucket the output is placed in... summon: true # do we want to visit the web sites and pull down the files mill: true context: cache: true contextmaps: - prefix: \"https://schema.org/\" file: \"./configs/schemaorg-current-https.jsonld\" - prefix: \"http://schema.org/\" file: \"./configs/schemaorg-current-https.jsonld\" summoner: after: \"\" # \"21 May 20 10:00 UTC\" mode: full # full || diff: If diff compare what we have currently in gleaner to sitemap, get only new, delete missing threads: 5 delay: # milliseconds (1000 = 1 second) to delay between calls (will FORCE threads to 1) headless: http://127.0.0.1:9222 # URL for headless see docs/headless millers: graph: true # will be built from sources.csv sources: - sourcetype: sitegraph name: aquadocs logo: \"\" url: https://oih.aquadocs.org/aquadocs.json headless: false pid: http://hdl.handle.net/1834/41372 propername: AquaDocs domain: https://aquadocs.org active: false - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: false A few things we need to look at. First, in the \"mino:\" section make sure the accessKey and secretKey here match the ones you have and set via your demo.env file. Next, lets look at the \"gleaner:\" section. We can set the runid to something. This is the ID for a run and it allows you to later make different runs and keep the resulting graphs organized. It can be set to any lower case string with no spaces. The miller and summon sections are true and we will leave them that way. It means we want Gleaner to both fetch the resources and process (mill) them. Now look at the \"miller:\" section when lets of pick what milling to do. Currently it is set with only graph set to true. Let's leave it that way for now. This means Gleaner will only attempt to make graph and not also run validation or generate prov reports for the process. The final section we need to look at is the \"sources:\" section. Here is where the fun is. While there are two types, sitegraph and sitemaps we will normally use sitemap type. A standard sitemap is below: sources: - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: true A sitegraph sources: - sourcetype: sitegraph name: aquadocs logo: \"\" url: https://oih.aquadocs.org/aquadocs.json headless: false pid: http://hdl.handle.net/1834/41372 propername: AquaDocs domain: https://aquadocs.org active: false A Google Drive sources: - sourcetype: googledrive name: ecrr_submitted logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd headless: false pid: \"\" propername: Earthcube Resource Registry domain: http://www.earthcube.org/resourceregistry/ active: true credentialsfile: configs/credentials/gleaner-331805-030e15e1d9c4.json other: {} These are the sources we wish to pull and process. Each source has a type, and 8 entries though at this time we no longer use the \"logo\" value. It was used in the past to provide a page showing all the sources and a logo for them. However, that's really just out of scope for what we want to do. You can leave it blank or set it to any value, it wont make a difference. The name is what you want to call this source. It should be one word (no space) and be lower case. The url value needs to point to the URL for the site map XML file. This will be created and served by the data provider. The headless value should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page. This is true of some sites and it is supported but not currently auto-detected. So you will need to know this and set it. For most place, this will be false. You can have as many sources as you wish. For an example look the configure file for the CDF Semantic Network at: https://github.com/gleanerio/CDFSemanticNetwork/blob/master/configs/cdf.yaml","title":"Gleaner Configuration"},{"location":"README_Configure_Template/","text":"Configure Using glcon and Templates \uf0c1 You do not need to have a container stack running to run glcon config But run glcon gleaner and glcon nabu , you will need to. OVERVIEW glcon Configuration generation \uf0c1 glcon config is used to create configuration files for gleaner and nabu The pattern is to intiialize a configuration directory, edit files, and generate new configuration files for gleaner and nabu. Inside a configuration, you will need to edit a localConfiguration file Edit/add sources in a csv listing, and generate the configurations. initialize a configuration directory \uf0c1 use glcon command can intialize a configuration directory, and allow for the generation of gleaner and nabu configurations glcon config init -cfgName test initializes a configuration in configs with name of 'test' Inside you will find test % ls gleaner_base.yaml readme.txt sources.csv nabu_base.yaml localConfig.yaml README_Configure_Template.md EDIT the files \uf0c1 Usually, you will only need to edit the localConfig.yaml and sources.csv The localConfig.yaml localConfig.yaml \uf0c1 --- minio: address: 0.0.0.0 # can be overridden with MINIO_ADDRESS port: 9000 # can be overridden with MINIO_PORT accessKey: worldsbestaccesskey # can be overridden with MINIO_ACCESS_KEY secretKey: worldsbestsecretkey # can be overridden with MINIO_SECRET_KEY ssl: false # can be overridden with MINIO_SSL bucket: gleaner # can be overridden with MINIO_BUCKET sparql: endpoint: http://localhost/blazegraph/namespace/earthcube/sparql s3: bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here. domain: us-east-1 #headless field in gleaner.summoner headless: http://127.0.0.1:9222 sourcesSource: type: csv location: sources.csv First, in the \"mino:\" section make sure the accessKey and secretKey here match the access keys for your minio. These can be overridden with the environent variables: * \"MINIO_ACCESS_KEY\" * \"MINIO_SECRET_KEY\" sourcesSource \uf0c1 sources.csv is utilized to generate the information needed to retrieve (summon), process (mill), upload (prefix), and cull old records (prune) This is done in order to facilitate the managing a list of sources, in a spreadsheet, rather than a yamil file. a csv file with the fields below This is designed to be edited in a spreadsheet, or exported by url as csv from a google spreadsheet hack,SourceType,Active,Name,ProperName,URL,Headless,Domain,PID,Logo,CredentialsFile 1,sitegraph,FALSE,aquadocs,AquaDocs,https://oih.aquadocs.org/aquadocs.json ,FALSE,https://aquadocs.org,http://hdl.handle.net/1834/41372,, 3,sitemap,TRUE,opentopography,OpenTopography,https://opentopography.org/sitemap.xml,FALSE,http://www.opentopography.org/,https://www.re3data.org/repository/r3d100010655,https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png, ,sitemap,TRUE,iris,IRIS,http://ds.iris.edu/files/sitemap.xml,FALSE,http://iris.edu,https://www.re3data.org/repository/r3d100010268,http://ds.iris.edu/static/img/layout/logos/iris_logo_shadow.png, Fields: 1. hack:a hack to make the fields are properly read. 2. SourceType : [sitemap, sitegraph, googledrive] type of source 3. Active: [TRUE,FALSE] is source active. 4. Name: short name of source. It should be one word (no space) and be lower case. 5. ProperName: Long name of source that will be added to organization record for provenance 6. URL: URL of sitemap or sitegraph. 7. Headless: [FALSE,TRUE] should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page. This is true of some sites and it is supported but not currently auto-detected. So you will need to know this and set it. For most place, this will be false. if the json-ld is generated in a page dynamically, then use , TRUE 8. Domain: 9. PID: a unique identifier for the source. Perfered that is is a research id. 10. Logo: while no longer used, logo of the source 11. CredentialsFile: (ONLY NEEDED FOR type:googledrive) environment variable pointing to a google api key. 12. any additional feilds you wish. This might be used to generate information about sources for a website. Configuration of your source \uf0c1 You configure the source in the localConfig.yaml (or override via the command line) # looks for Mysources.csv in configuration directory sourcesSource: type: csv location: Mysources.csv This can also be a remote url starting with http:// or https:// # pulls from a google sheet sourcesSource: type: csv location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&sheet={sheet_name} If you start the name with a '/' a full path to the file is assumed. # file outside of local configuration directory sourcesSource: type: csv location: /home/user/ourSources.csv GENERATE the configuration files \uf0c1 glcon generate -cfgName test This will generate files 'gleaner' and 'nabu' and make copies of the existing configuration files Some Gleaner Configuration details \uf0c1 This is a summary of a few portions of the configuration files generated. More details are at: GleanerConfiguration.md Open the file 'gleaner' , and you will see is actually quite a bit information this file, but for this starting demo only a few things we need to worry about. The default file will look like: --- minio: address: 0.0.0.0 port: 9000 accessKey: worldsbestaccesskey secretKey: worldsbestsecretkey ssl: false bucket: gleaner gleaner: runid: runX # this will be the bucket the output is placed in... summon: true # do we want to visit the web sites and pull down the files mill: true context: cache: true contextmaps: - prefix: \"https://schema.org/\" file: \"./configs/schemaorg-current-https.jsonld\" - prefix: \"http://schema.org/\" file: \"./configs/schemaorg-current-https.jsonld\" summoner: after: \"\" # \"21 May 20 10:00 UTC\" mode: full # full || diff: If diff compare what we have currently in gleaner to sitemap, get only new, delete missing threads: 5 delay: # milliseconds (1000 = 1 second) to delay between calls (will FORCE threads to 1) headless: http://127.0.0.1:9222 # URL for headless see docs/headless millers: graph: true # will be built from sources.csv sources: - sourcetype: sitegraph name: aquadocs logo: \"\" url: https://oih.aquadocs.org/aquadocs.json headless: false pid: http://hdl.handle.net/1834/41372 propername: AquaDocs domain: https://aquadocs.org active: false - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: false A few things we need to look at. First, in the \"minio:\" section make sure the accessKey and secretKey here are the ones you utlize. Note: blank these out, and used environment variables (TODO:Need to describe them) Next, lets look at the \"gleaner:\" section. We can set the runid to something. This is the ID for a run and it allows you to later make different runs and keep the resulting graphs organized. It can be set to any lower case string with no spaces. The \"context:\" section is about the JSON-LD context, which is a top-level object in each JSON-LD metadata document. Most of the time, it'll look like the example context here . By default, Gleaner will look for common mistakes in the JSON-LD context specification and fix them up. Setting strict: true here will disable those fixup operations. It might make things run a little faster to do this. The miller and summon sections are true and we will leave them that way. It means we want Gleaner to both fetch the resources and process (mill) them. Now look at the \"miller:\" section when lets of pick what milling to do. Currently it is set with only graph set to true. Let's leave it that way for now. This means Gleaner will only attempt to make graph and not also run validation or generate prov reports for the process. The final section we need to look at is the \"sources:\" section. Here is where the fun is. While the most common type of source is a sitemap , there are other types available, and examples of each are below. A standard sitemap is below: sources: - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: true A sitegraph sources: - sourcetype: sitegraph name: aquadocs logo: \"\" url: https://oih.aquadocs.org/aquadocs.json headless: false pid: http://hdl.handle.net/1834/41372 propername: AquaDocs domain: https://aquadocs.org active: false A google Drive sources: - sourcetype: googledrive name: ecrr_submitted logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd headless: false pid: \"\" propername: Earthcube Resource Registry domain: http://www.earthcube.org/resourceregistry/ active: true credentialsfile: configs/credentials/gleaner-331805-030e15e1d9c4.json other: {} A csv TODO: I know this exists but I don't know what it looks like A robots.txt url (which can have links to multiple sitemaps) sources: - name: npdc sourcetype: robots headless: false url: https://npdc.nl/robots.txt properName: Netherlands Polar Data Center domain: https://npdc.nl active: false These are the sources we wish to pull and process. Each source has a type, and 8 entries though at this time we no longer use the \"logo\" value. It was used in the past to provide a page showing all the sources and a logo for them. However, that's really just out of scope for what we want to do. You can leave it blank or set it to any value, it wont make a difference. The name is what you want to call this source. It should be one word (no space) and be lower case. The url value needs to point to the URL for the site map XML file. This will be created and served by the data provider. The headless value should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page. This is true of some sites and it is supported but not currently auto-detected. So you will need to know this and set it. For most place, this will be false. You can have as many sources as you wish. For an example look the configure file for the CDF Semantic Network at: https://github.com/gleanerio/CDFSemanticNetwork/blob/master/configs/cdf.yaml","title":"Configure Using glcon and Templates"},{"location":"README_Configure_Template/#configure-using-glcon-and-templates","text":"You do not need to have a container stack running to run glcon config But run glcon gleaner and glcon nabu , you will need to.","title":"Configure Using glcon and Templates"},{"location":"README_Configure_Template/#overview-glcon-configuration-generation","text":"glcon config is used to create configuration files for gleaner and nabu The pattern is to intiialize a configuration directory, edit files, and generate new configuration files for gleaner and nabu. Inside a configuration, you will need to edit a localConfiguration file Edit/add sources in a csv listing, and generate the configurations.","title":"OVERVIEW glcon Configuration generation"},{"location":"README_Configure_Template/#initialize-a-configuration-directory","text":"use glcon command can intialize a configuration directory, and allow for the generation of gleaner and nabu configurations glcon config init -cfgName test initializes a configuration in configs with name of 'test' Inside you will find test % ls gleaner_base.yaml readme.txt sources.csv nabu_base.yaml localConfig.yaml README_Configure_Template.md","title":"initialize a configuration directory"},{"location":"README_Configure_Template/#edit-the-files","text":"Usually, you will only need to edit the localConfig.yaml and sources.csv The localConfig.yaml","title":"EDIT the files"},{"location":"README_Configure_Template/#localconfigyaml","text":"--- minio: address: 0.0.0.0 # can be overridden with MINIO_ADDRESS port: 9000 # can be overridden with MINIO_PORT accessKey: worldsbestaccesskey # can be overridden with MINIO_ACCESS_KEY secretKey: worldsbestsecretkey # can be overridden with MINIO_SECRET_KEY ssl: false # can be overridden with MINIO_SSL bucket: gleaner # can be overridden with MINIO_BUCKET sparql: endpoint: http://localhost/blazegraph/namespace/earthcube/sparql s3: bucket: gleaner # sync with above... can be overridden with MINIO_BUCKET... get's zapped if it's not here. domain: us-east-1 #headless field in gleaner.summoner headless: http://127.0.0.1:9222 sourcesSource: type: csv location: sources.csv First, in the \"mino:\" section make sure the accessKey and secretKey here match the access keys for your minio. These can be overridden with the environent variables: * \"MINIO_ACCESS_KEY\" * \"MINIO_SECRET_KEY\"","title":"localConfig.yaml"},{"location":"README_Configure_Template/#sourcessource","text":"sources.csv is utilized to generate the information needed to retrieve (summon), process (mill), upload (prefix), and cull old records (prune) This is done in order to facilitate the managing a list of sources, in a spreadsheet, rather than a yamil file. a csv file with the fields below This is designed to be edited in a spreadsheet, or exported by url as csv from a google spreadsheet hack,SourceType,Active,Name,ProperName,URL,Headless,Domain,PID,Logo,CredentialsFile 1,sitegraph,FALSE,aquadocs,AquaDocs,https://oih.aquadocs.org/aquadocs.json ,FALSE,https://aquadocs.org,http://hdl.handle.net/1834/41372,, 3,sitemap,TRUE,opentopography,OpenTopography,https://opentopography.org/sitemap.xml,FALSE,http://www.opentopography.org/,https://www.re3data.org/repository/r3d100010655,https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png, ,sitemap,TRUE,iris,IRIS,http://ds.iris.edu/files/sitemap.xml,FALSE,http://iris.edu,https://www.re3data.org/repository/r3d100010268,http://ds.iris.edu/static/img/layout/logos/iris_logo_shadow.png, Fields: 1. hack:a hack to make the fields are properly read. 2. SourceType : [sitemap, sitegraph, googledrive] type of source 3. Active: [TRUE,FALSE] is source active. 4. Name: short name of source. It should be one word (no space) and be lower case. 5. ProperName: Long name of source that will be added to organization record for provenance 6. URL: URL of sitemap or sitegraph. 7. Headless: [FALSE,TRUE] should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page. This is true of some sites and it is supported but not currently auto-detected. So you will need to know this and set it. For most place, this will be false. if the json-ld is generated in a page dynamically, then use , TRUE 8. Domain: 9. PID: a unique identifier for the source. Perfered that is is a research id. 10. Logo: while no longer used, logo of the source 11. CredentialsFile: (ONLY NEEDED FOR type:googledrive) environment variable pointing to a google api key. 12. any additional feilds you wish. This might be used to generate information about sources for a website.","title":"sourcesSource"},{"location":"README_Configure_Template/#configuration-of-your-source","text":"You configure the source in the localConfig.yaml (or override via the command line) # looks for Mysources.csv in configuration directory sourcesSource: type: csv location: Mysources.csv This can also be a remote url starting with http:// or https:// # pulls from a google sheet sourcesSource: type: csv location: https://docs.google.com/spreadsheets/d/{key}/gviz/tq?tqx=out:csv&sheet={sheet_name} If you start the name with a '/' a full path to the file is assumed. # file outside of local configuration directory sourcesSource: type: csv location: /home/user/ourSources.csv","title":"Configuration of your source"},{"location":"README_Configure_Template/#generate-the-configuration-files","text":"glcon generate -cfgName test This will generate files 'gleaner' and 'nabu' and make copies of the existing configuration files","title":"GENERATE the configuration files"},{"location":"README_Configure_Template/#some-gleaner-configuration-details","text":"This is a summary of a few portions of the configuration files generated. More details are at: GleanerConfiguration.md Open the file 'gleaner' , and you will see is actually quite a bit information this file, but for this starting demo only a few things we need to worry about. The default file will look like: --- minio: address: 0.0.0.0 port: 9000 accessKey: worldsbestaccesskey secretKey: worldsbestsecretkey ssl: false bucket: gleaner gleaner: runid: runX # this will be the bucket the output is placed in... summon: true # do we want to visit the web sites and pull down the files mill: true context: cache: true contextmaps: - prefix: \"https://schema.org/\" file: \"./configs/schemaorg-current-https.jsonld\" - prefix: \"http://schema.org/\" file: \"./configs/schemaorg-current-https.jsonld\" summoner: after: \"\" # \"21 May 20 10:00 UTC\" mode: full # full || diff: If diff compare what we have currently in gleaner to sitemap, get only new, delete missing threads: 5 delay: # milliseconds (1000 = 1 second) to delay between calls (will FORCE threads to 1) headless: http://127.0.0.1:9222 # URL for headless see docs/headless millers: graph: true # will be built from sources.csv sources: - sourcetype: sitegraph name: aquadocs logo: \"\" url: https://oih.aquadocs.org/aquadocs.json headless: false pid: http://hdl.handle.net/1834/41372 propername: AquaDocs domain: https://aquadocs.org active: false - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: false A few things we need to look at. First, in the \"minio:\" section make sure the accessKey and secretKey here are the ones you utlize. Note: blank these out, and used environment variables (TODO:Need to describe them) Next, lets look at the \"gleaner:\" section. We can set the runid to something. This is the ID for a run and it allows you to later make different runs and keep the resulting graphs organized. It can be set to any lower case string with no spaces. The \"context:\" section is about the JSON-LD context, which is a top-level object in each JSON-LD metadata document. Most of the time, it'll look like the example context here . By default, Gleaner will look for common mistakes in the JSON-LD context specification and fix them up. Setting strict: true here will disable those fixup operations. It might make things run a little faster to do this. The miller and summon sections are true and we will leave them that way. It means we want Gleaner to both fetch the resources and process (mill) them. Now look at the \"miller:\" section when lets of pick what milling to do. Currently it is set with only graph set to true. Let's leave it that way for now. This means Gleaner will only attempt to make graph and not also run validation or generate prov reports for the process. The final section we need to look at is the \"sources:\" section. Here is where the fun is. While the most common type of source is a sitemap , there are other types available, and examples of each are below. A standard sitemap is below: sources: - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: true A sitegraph sources: - sourcetype: sitegraph name: aquadocs logo: \"\" url: https://oih.aquadocs.org/aquadocs.json headless: false pid: http://hdl.handle.net/1834/41372 propername: AquaDocs domain: https://aquadocs.org active: false A google Drive sources: - sourcetype: googledrive name: ecrr_submitted logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd headless: false pid: \"\" propername: Earthcube Resource Registry domain: http://www.earthcube.org/resourceregistry/ active: true credentialsfile: configs/credentials/gleaner-331805-030e15e1d9c4.json other: {} A csv TODO: I know this exists but I don't know what it looks like A robots.txt url (which can have links to multiple sitemaps) sources: - name: npdc sourcetype: robots headless: false url: https://npdc.nl/robots.txt properName: Netherlands Polar Data Center domain: https://npdc.nl active: false These are the sources we wish to pull and process. Each source has a type, and 8 entries though at this time we no longer use the \"logo\" value. It was used in the past to provide a page showing all the sources and a logo for them. However, that's really just out of scope for what we want to do. You can leave it blank or set it to any value, it wont make a difference. The name is what you want to call this source. It should be one word (no space) and be lower case. The url value needs to point to the URL for the site map XML file. This will be created and served by the data provider. The headless value should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page. This is true of some sites and it is supported but not currently auto-detected. So you will need to know this and set it. For most place, this will be false. You can have as many sources as you wish. For an example look the configure file for the CDF Semantic Network at: https://github.com/gleanerio/CDFSemanticNetwork/blob/master/configs/cdf.yaml","title":"Some Gleaner Configuration details"},{"location":"README_glcon/","text":"Gleaner Console (glcon) \uf0c1 About \uf0c1 Console about merging tasks for gleaner and nabu into a single command. It creates configuration files, can setup the triplestore, run gleaner and run nabu glcon --help Workflow \uf0c1 User initializes configuration directory: glcon config init --cfgName name user edits files in directory user generates configuration: glcon config generate --cfgName name User runs gleaner setup: glcon gleaner setup --cfgName name user runs gleaner: glcon gleaner batch --cfgName name user runs nabu prefix: glcon nabu prefix --cfgName name user runs nabu prune: glcon nabu prune --cfgName name User edits configuration, and regenerates configurations, runs gleaner and nabu configuration building: \uf0c1 glcon config init --cfgName name will create a directory in configs, with base configs to modify configurtion is generated in configs/{name} User will need to edit the servers.yaml, and sources.csv. They may also need to edit gleaner_base.yaml, and nabu.base.yaml glcon config generate --cfgName name will generate gleaner and nabu configurations, and make copies (one per day, for now) The routine will merge information from servers.yaml with gleaner_base.yaml to create gleaner The routine will merge information from servers.yaml with nabu_base.yaml to create nBU The routine will use sources.csv to create a list of sources to process in the output of gleaner and nabu Environmental variable substition will occur: (need list of env avaiables) Executing Gleaner \uf0c1 glcon gleaner setup --cfgName name Reads the gleaner configuration file, and checks if the s3 minio service is available, and creates buckets if needed Environmental variable substition will occur: (need list of env avaiables) glcon gleaner batch --cfgName name Reads the gleaner configuration file, and executes gleaner. Environmental variable substition will occur: (need list of env avaiables) Executing Nabu \uf0c1 glcon nabu prefix --cfgName name Load graphs from prefix to triplestore Environmental variable substition will occur: (need list of env avaiables) glcon nabu prune --cfgName name Prune graphs in triplestore not in objectVal store Reads the gleaner configuration file, and executes gleaner. Environmental variable substition will occur: (need list of env avaiables) Environment variables \uf0c1 (\"minio.address\", \"MINIO_ADDRESS\") (\"minio.port\", \"MINIO_PORT\") (\"minio.ssl\", \"MINIO_USE_SSL\") (\"minio.accesskey\", \"MINIO_ACCESS_KEY\") (\"minio.secretkey\", \"MINIO_SECRET_KEY\") (\"minio.bucket\", \"MINIO_BUCKET\") (\"minio.domain\", \"S3_DOMAIN\") (\"sparql.endpoint\", \"SPARQL_ENDPOINT\") (\"sparql.authenticate\", \"SPARQL_AUTHENTICATE\") (\"sparql.username\", \"SPARQL_USERNAME\") (\"sparql.password\", \"SPARQL_PASSWORD\") (\"gleaner.headless\", \"GLEANER_HEADLESS_ENDPOINT\") (\"gleaner.threads\", \"GLEANER_THREADS\") (\"gleaner.mode\", \"GLEANER_MODE\") Notes \uf0c1 Create configuration files for Gleaner and Nabu: * init a config files for gleaner and nabu * generate templates to a config directory * example servers and service, (tikka? use flag?). * example sources.csv * example gleaner * example nabu * generate/update a config file for gleaner/nabu * merge mino and sources configurations * check setup * read a mino config * read a csv file with headers to manage 'sources' * validate csv format * pull, push, check local config to minio config configuration building: glcon config init --cfgName name will create a directory in configs, with base configs to modify glcon config generate --cfgName name will generate gleaner and nabu configurations, and make copies Configuration \uf0c1 glcon config init --cfgName X glcon config generate --cfgName X glcon config validate --cfgName X Gleaner \uf0c1 glcon gleaner setup --cfgName X glcon gleaner batch --cfgName X Nabu \uf0c1 glcon nabu prefix --cfgName X glcon nabu prune --cfgName X glcon nabu object --cfgName X Refs \uf0c1 https://farazdagi.com/2014/rest-and-long-running-jobs/ Cobra Golang commander","title":"Gleaner Console  (glcon)"},{"location":"README_glcon/#gleaner-console-glcon","text":"","title":"Gleaner Console  (glcon)"},{"location":"README_glcon/#about","text":"Console about merging tasks for gleaner and nabu into a single command. It creates configuration files, can setup the triplestore, run gleaner and run nabu glcon --help","title":"About"},{"location":"README_glcon/#workflow","text":"User initializes configuration directory: glcon config init --cfgName name user edits files in directory user generates configuration: glcon config generate --cfgName name User runs gleaner setup: glcon gleaner setup --cfgName name user runs gleaner: glcon gleaner batch --cfgName name user runs nabu prefix: glcon nabu prefix --cfgName name user runs nabu prune: glcon nabu prune --cfgName name User edits configuration, and regenerates configurations, runs gleaner and nabu","title":"Workflow"},{"location":"README_glcon/#configuration-building","text":"glcon config init --cfgName name will create a directory in configs, with base configs to modify configurtion is generated in configs/{name} User will need to edit the servers.yaml, and sources.csv. They may also need to edit gleaner_base.yaml, and nabu.base.yaml glcon config generate --cfgName name will generate gleaner and nabu configurations, and make copies (one per day, for now) The routine will merge information from servers.yaml with gleaner_base.yaml to create gleaner The routine will merge information from servers.yaml with nabu_base.yaml to create nBU The routine will use sources.csv to create a list of sources to process in the output of gleaner and nabu Environmental variable substition will occur: (need list of env avaiables)","title":"configuration building:"},{"location":"README_glcon/#executing-gleaner","text":"glcon gleaner setup --cfgName name Reads the gleaner configuration file, and checks if the s3 minio service is available, and creates buckets if needed Environmental variable substition will occur: (need list of env avaiables) glcon gleaner batch --cfgName name Reads the gleaner configuration file, and executes gleaner. Environmental variable substition will occur: (need list of env avaiables)","title":"Executing Gleaner"},{"location":"README_glcon/#executing-nabu","text":"glcon nabu prefix --cfgName name Load graphs from prefix to triplestore Environmental variable substition will occur: (need list of env avaiables) glcon nabu prune --cfgName name Prune graphs in triplestore not in objectVal store Reads the gleaner configuration file, and executes gleaner. Environmental variable substition will occur: (need list of env avaiables)","title":"Executing Nabu"},{"location":"README_glcon/#environment-variables","text":"(\"minio.address\", \"MINIO_ADDRESS\") (\"minio.port\", \"MINIO_PORT\") (\"minio.ssl\", \"MINIO_USE_SSL\") (\"minio.accesskey\", \"MINIO_ACCESS_KEY\") (\"minio.secretkey\", \"MINIO_SECRET_KEY\") (\"minio.bucket\", \"MINIO_BUCKET\") (\"minio.domain\", \"S3_DOMAIN\") (\"sparql.endpoint\", \"SPARQL_ENDPOINT\") (\"sparql.authenticate\", \"SPARQL_AUTHENTICATE\") (\"sparql.username\", \"SPARQL_USERNAME\") (\"sparql.password\", \"SPARQL_PASSWORD\") (\"gleaner.headless\", \"GLEANER_HEADLESS_ENDPOINT\") (\"gleaner.threads\", \"GLEANER_THREADS\") (\"gleaner.mode\", \"GLEANER_MODE\")","title":"Environment variables"},{"location":"README_glcon/#notes","text":"Create configuration files for Gleaner and Nabu: * init a config files for gleaner and nabu * generate templates to a config directory * example servers and service, (tikka? use flag?). * example sources.csv * example gleaner * example nabu * generate/update a config file for gleaner/nabu * merge mino and sources configurations * check setup * read a mino config * read a csv file with headers to manage 'sources' * validate csv format * pull, push, check local config to minio config configuration building: glcon config init --cfgName name will create a directory in configs, with base configs to modify glcon config generate --cfgName name will generate gleaner and nabu configurations, and make copies","title":"Notes"},{"location":"README_glcon/#configuration","text":"glcon config init --cfgName X glcon config generate --cfgName X glcon config validate --cfgName X","title":"Configuration"},{"location":"README_glcon/#gleaner","text":"glcon gleaner setup --cfgName X glcon gleaner batch --cfgName X","title":"Gleaner"},{"location":"README_glcon/#nabu","text":"glcon nabu prefix --cfgName X glcon nabu prune --cfgName X glcon nabu object --cfgName X","title":"Nabu"},{"location":"README_glcon/#refs","text":"https://farazdagi.com/2014/rest-and-long-running-jobs/ Cobra Golang commander","title":"Refs"},{"location":"SourceGithubAsAJsonLDSource/","text":"Github \uf0c1 You can use github as a repository for jsonLd files. Put your jsonln files in a repository. Then you can use a github action to generate a sitemap and push to github pages. Warning... if you use githubb pages, this will need to be modified. It can wipe out existing pages To generate a sitemap using an action 1. create a directory: .github/workflows 2. add file sitemap.yaml from below: sitemap action workflow file 3. modify: * with.base-url-path * with.path-to-root 4. commit 5. go to actions, see that actions run. 6. enable github pages 7. See if your sitemap is at location: https://{org}.github.io/{repo}/{path}/sitemap.xml sitemap action workflow file \uf0c1 This example publishes a sitemap for earhtcube/GeoCODES-Metadata from the metadata/Datasets directory. GeoCODES-Metadata -- metadata -- Dataset -- jsonld files We want to point at the raw Json files are on the main branch.go to metadata/dataset , click on a file, and select RAW, you will see URL starting with this is our base-url-path https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/main/metadata/Dataset/ our path-to-root is metadata/Dataset For this Generated sitemap below, the sitemap url for a gleaner source will be: https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/sitemap.xml name: Generate xml sitemap on: push: branches: [ main ] jobs: sitemap_job: runs-on: ubuntu-latest name: Generate a sitemap steps: - name: Checkout the repo uses: actions/checkout@v2 with: fetch-depth: 0 - name: Generate the dataset sitemap id: sitemapdataset uses: cicirello/generate-sitemap@v1 with: base-url-path: https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/main/metadata/Dataset/ path-to-root: metadata/Dataset include-pdf: false additional-extensions: json jsonld - name: Output dataset stats run: | echo \"sitemap-path = ${{ steps.sitemapdataset.outputs.sitemap-path }}\" echo \"url-count = ${{ steps.sitemapdataset.outputs.url-count }}\" echo \"excluded-count = ${{ steps.sitemapdataset.outputs.excluded-count }}\" - name: push to gh pages uses: JamesIves/github-pages-deploy-action@4.1.6 with: branch: gh-pages folder: . add this to sources \"hack\",\"SourceType\",\"Active\",\"Name\",\"ProperName\",\"URL\",\"Headless\",\"Domain\",\"PID\",\"Logo\" \"52\",\"sitemap\",\"TRUE\",\"ecrr_examples\",\"Earthcube Resource Registry Examples\",\"https://raw.githubusercontent.com/earthcube/ecrro/master/Examples/sitemap.xml\",\"FALSE\",\"http://www.earthcube.org/resourceregistry/examples\",\"\",\"https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png\" \"\",\"sitemap\",\"TRUE\",\"geocodes_examples\",\"GeoCodes Tools Examples\",\"https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/main/sitemap.xml\",\"FALSE\",\"https://github.com/earthcube/GeoCODES-Metadata/\",\"\",\"\"","title":"Github"},{"location":"SourceGithubAsAJsonLDSource/#github","text":"You can use github as a repository for jsonLd files. Put your jsonln files in a repository. Then you can use a github action to generate a sitemap and push to github pages. Warning... if you use githubb pages, this will need to be modified. It can wipe out existing pages To generate a sitemap using an action 1. create a directory: .github/workflows 2. add file sitemap.yaml from below: sitemap action workflow file 3. modify: * with.base-url-path * with.path-to-root 4. commit 5. go to actions, see that actions run. 6. enable github pages 7. See if your sitemap is at location: https://{org}.github.io/{repo}/{path}/sitemap.xml","title":"Github"},{"location":"SourceGithubAsAJsonLDSource/#sitemap-action-workflow-file","text":"This example publishes a sitemap for earhtcube/GeoCODES-Metadata from the metadata/Datasets directory. GeoCODES-Metadata -- metadata -- Dataset -- jsonld files We want to point at the raw Json files are on the main branch.go to metadata/dataset , click on a file, and select RAW, you will see URL starting with this is our base-url-path https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/main/metadata/Dataset/ our path-to-root is metadata/Dataset For this Generated sitemap below, the sitemap url for a gleaner source will be: https://earthcube.github.io/GeoCODES-Metadata/metadata/Dataset/sitemap.xml name: Generate xml sitemap on: push: branches: [ main ] jobs: sitemap_job: runs-on: ubuntu-latest name: Generate a sitemap steps: - name: Checkout the repo uses: actions/checkout@v2 with: fetch-depth: 0 - name: Generate the dataset sitemap id: sitemapdataset uses: cicirello/generate-sitemap@v1 with: base-url-path: https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/main/metadata/Dataset/ path-to-root: metadata/Dataset include-pdf: false additional-extensions: json jsonld - name: Output dataset stats run: | echo \"sitemap-path = ${{ steps.sitemapdataset.outputs.sitemap-path }}\" echo \"url-count = ${{ steps.sitemapdataset.outputs.url-count }}\" echo \"excluded-count = ${{ steps.sitemapdataset.outputs.excluded-count }}\" - name: push to gh pages uses: JamesIves/github-pages-deploy-action@4.1.6 with: branch: gh-pages folder: . add this to sources \"hack\",\"SourceType\",\"Active\",\"Name\",\"ProperName\",\"URL\",\"Headless\",\"Domain\",\"PID\",\"Logo\" \"52\",\"sitemap\",\"TRUE\",\"ecrr_examples\",\"Earthcube Resource Registry Examples\",\"https://raw.githubusercontent.com/earthcube/ecrro/master/Examples/sitemap.xml\",\"FALSE\",\"http://www.earthcube.org/resourceregistry/examples\",\"\",\"https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png\" \"\",\"sitemap\",\"TRUE\",\"geocodes_examples\",\"GeoCodes Tools Examples\",\"https://raw.githubusercontent.com/earthcube/GeoCODES-Metadata/main/sitemap.xml\",\"FALSE\",\"https://github.com/earthcube/GeoCODES-Metadata/\",\"\",\"\"","title":"sitemap action workflow file"},{"location":"SourceGoogleDrive/","text":"Using a google drive as a JSONLD source \uf0c1 This involves using an API key, and an environment variable. An enviroment variable is requeired, since multiple sources could use different api keys. (and security... can't check in an env variable ;) ) (use case is Earthcube Resource Registry, where google form results are converted to jsonld files.) \"hack\",\"SourceType\",\"Active\",\"Name\",\"ProperName\",\"URL\",\"Headless\",\"Domain\",\"PID\",\"Logo\" \"51\",\"googledrive\",\"TRUE\",\"ecrr_submitted\",\"Earthcube Resource Registry\",\"https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd\",\"FALSE\",\"http://www.earthcube.org/resourceregistry/\",\"\",\"https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png\" - sourcetype: googledrive name: ecrr_submitted logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd headless: false pid: \"\" propername: Earthcube Resource Registry domain: http://www.earthcube.org/resourceregistry/ active: true CredentialsEnv: GOOGLEAPIAUTH other: {} The credentials files is located at: setenv GOOGLEAPIAUTH = credentials/filename.json Credentials \uf0c1 A repository needs to provide a server_account json file https://help.talend.com/r/E3i03eb7IpvsigwC58fxQg/ol2OwTHmFbDiMjQl3ES5QA { \"type\": \"service_account\", \"project_id\": \"gleaner-\", \"private_key_id\": \"ke\", \"private_key\": \"-----BEGIN PRIVATE KEY----------END PRIVATE KEY-----\\n\", \"client_email\": \".iam.gserviceaccount.com\", \"client_id\": \"\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://oauth2.googleapis.com/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/gserviceaccount.com\" }","title":"SourceGoogleDrive"},{"location":"SourceGoogleDrive/#using-a-google-drive-as-a-jsonld-source","text":"This involves using an API key, and an environment variable. An enviroment variable is requeired, since multiple sources could use different api keys. (and security... can't check in an env variable ;) ) (use case is Earthcube Resource Registry, where google form results are converted to jsonld files.) \"hack\",\"SourceType\",\"Active\",\"Name\",\"ProperName\",\"URL\",\"Headless\",\"Domain\",\"PID\",\"Logo\" \"51\",\"googledrive\",\"TRUE\",\"ecrr_submitted\",\"Earthcube Resource Registry\",\"https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd\",\"FALSE\",\"http://www.earthcube.org/resourceregistry/\",\"\",\"https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png\" - sourcetype: googledrive name: ecrr_submitted logo: https://www.earthcube.org/sites/default/files/doc-repository/logo_earthcube_full_horizontal.png url: https://drive.google.com/drive/u/0/folders/1TacUQqjpBbGsPQ8JPps47lBXMQsNBRnd headless: false pid: \"\" propername: Earthcube Resource Registry domain: http://www.earthcube.org/resourceregistry/ active: true CredentialsEnv: GOOGLEAPIAUTH other: {} The credentials files is located at: setenv GOOGLEAPIAUTH = credentials/filename.json","title":"Using a google drive as a JSONLD source"},{"location":"SourceGoogleDrive/#credentials","text":"A repository needs to provide a server_account json file https://help.talend.com/r/E3i03eb7IpvsigwC58fxQg/ol2OwTHmFbDiMjQl3ES5QA { \"type\": \"service_account\", \"project_id\": \"gleaner-\", \"private_key_id\": \"ke\", \"private_key\": \"-----BEGIN PRIVATE KEY----------END PRIVATE KEY-----\\n\", \"client_email\": \".iam.gserviceaccount.com\", \"client_id\": \"\", \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\", \"token_uri\": \"https://oauth2.googleapis.com/token\", \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\", \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/gserviceaccount.com\" }","title":"Credentials"},{"location":"SourceSitemap/","text":"Using a Sitemap as Source \uf0c1 This is fairly simple. * name, propername * Url of the sitemap, * select an identifier as a PID, usually link to a repository of repositories, or a doi * domain (WHAT IS THE DOMAIN) * headless: if the repository generates the JSON dymically in the client, then true. * active: true if you want this to be utilized in when running. We have a list of all partners, but all partners to not have JSONLD or sitemaps. \"hack\",\"SourceType\",\"Active\",\"Name\",\"ProperName\",\"URL\",\"Headless\",\"Domain\",\"PID\",\"Logo\" \"3\",\"sitemap\",\"TRUE\",\"opentopography\",\"OpenTopography\",\"https://opentopography.org/sitemap.xml\",\"FALSE\",\"http://www.opentopography.org/\",\"https://www.re3data.org/repository/r3d100010655\",\"https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png\" A standard sitemap is below: sources: - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: true","title":"SourceSitemap"},{"location":"SourceSitemap/#using-a-sitemap-as-source","text":"This is fairly simple. * name, propername * Url of the sitemap, * select an identifier as a PID, usually link to a repository of repositories, or a doi * domain (WHAT IS THE DOMAIN) * headless: if the repository generates the JSON dymically in the client, then true. * active: true if you want this to be utilized in when running. We have a list of all partners, but all partners to not have JSONLD or sitemaps. \"hack\",\"SourceType\",\"Active\",\"Name\",\"ProperName\",\"URL\",\"Headless\",\"Domain\",\"PID\",\"Logo\" \"3\",\"sitemap\",\"TRUE\",\"opentopography\",\"OpenTopography\",\"https://opentopography.org/sitemap.xml\",\"FALSE\",\"http://www.opentopography.org/\",\"https://www.re3data.org/repository/r3d100010655\",\"https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png\" A standard sitemap is below: sources: - sourcetype: sitemap name: opentopography logo: https://opentopography.org/sites/opentopography.org/files/ot_transp_logo_2.png url: https://opentopography.org/sitemap.xml headless: false pid: https://www.re3data.org/repository/r3d100010655 propername: OpenTopography domain: http://www.opentopography.org/ active: true","title":"Using a Sitemap as Source"},{"location":"buildrun/","text":"More \uf0c1 The Summoner , which uses site map files to access and parse facility resources pages. Summoner places the results of these calls into a S3 API compliant storage. The Miller , which takes the JSON-LD documents pulled and stored by summoner and runs them through various millers. These millers can do various things. The current millers are: text: build a text index in raw bleve spatial: parse and build a spatial index using a geohash server graph: convert the JSON-LD to RDF for SPARQL queries A set of other millers exist that are more experimental tika: access the actual data files referneced by the JSON-LD and process through Apache Tika. The extracted text is then indexed in text system allowing full text search on the document contents. blast: like text, but using the blast package built on bleve fdptika: like tika, but using Frictionless Data Packages ftpgraph: like graph, but pulling JSON-LD files from Frictionless Data Packages prov: build a basic prov graph from the overall gleaner process shacl: validate the facility resoruces against defined SHACL shape graphs Building \uf0c1 Gleaner is built in 100% Go which can be found and installed from htttps://golang.org . Gleaner uses Go Modules so all dependencies will be downloaded at compile time. So a simple; GOOS=linux GOARCH=amd64 CGO_ENABLED=0 env go build -o gleaner in cmd/gleaner will be enough to resolve dependencies and build the binary. There is also a Makefile with basic commands if you have and use Make. Note the docker push will need to be edited to support your setup. There is a docker build file as well in the deployments directory if you wish to use the tool that way. However, there are some issues with using Gleaner in Docker/ How to run (or at least try..., this is still a work in progress) \uf0c1 A key focus of current develoipment is to make it easy for groups to run Gleaner locally as a means to test and validate their structured data publishing workflow. Running \uf0c1 Some early documentation on running gleaner can be found at: Running Gleaner . Validation (SHACL Shapes) \uf0c1 Work on the validation of data graphs using W3C SHACL shape graphs is taing place in the GeoShapes repository . Gleaner leverages the pySHACL Python package to perform the actual validation. Profiling (for dev work) \uf0c1 You can profile runs with go tool pprof --pdf gleaner /tmp/profile317320184/cpu.pprof > profileRun1.pdf Example CPU and Memory profile of a recent release.","title":"Buildrun"},{"location":"buildrun/#more","text":"The Summoner , which uses site map files to access and parse facility resources pages. Summoner places the results of these calls into a S3 API compliant storage. The Miller , which takes the JSON-LD documents pulled and stored by summoner and runs them through various millers. These millers can do various things. The current millers are: text: build a text index in raw bleve spatial: parse and build a spatial index using a geohash server graph: convert the JSON-LD to RDF for SPARQL queries A set of other millers exist that are more experimental tika: access the actual data files referneced by the JSON-LD and process through Apache Tika. The extracted text is then indexed in text system allowing full text search on the document contents. blast: like text, but using the blast package built on bleve fdptika: like tika, but using Frictionless Data Packages ftpgraph: like graph, but pulling JSON-LD files from Frictionless Data Packages prov: build a basic prov graph from the overall gleaner process shacl: validate the facility resoruces against defined SHACL shape graphs","title":"More"},{"location":"buildrun/#building","text":"Gleaner is built in 100% Go which can be found and installed from htttps://golang.org . Gleaner uses Go Modules so all dependencies will be downloaded at compile time. So a simple; GOOS=linux GOARCH=amd64 CGO_ENABLED=0 env go build -o gleaner in cmd/gleaner will be enough to resolve dependencies and build the binary. There is also a Makefile with basic commands if you have and use Make. Note the docker push will need to be edited to support your setup. There is a docker build file as well in the deployments directory if you wish to use the tool that way. However, there are some issues with using Gleaner in Docker/","title":"Building"},{"location":"buildrun/#how-to-run-or-at-least-try-this-is-still-a-work-in-progress","text":"A key focus of current develoipment is to make it easy for groups to run Gleaner locally as a means to test and validate their structured data publishing workflow.","title":"How to run (or at least try..., this is still a work in progress)"},{"location":"buildrun/#running","text":"Some early documentation on running gleaner can be found at: Running Gleaner .","title":"Running"},{"location":"buildrun/#validation-shacl-shapes","text":"Work on the validation of data graphs using W3C SHACL shape graphs is taing place in the GeoShapes repository . Gleaner leverages the pySHACL Python package to perform the actual validation.","title":"Validation (SHACL Shapes)"},{"location":"buildrun/#profiling-for-dev-work","text":"You can profile runs with go tool pprof --pdf gleaner /tmp/profile317320184/cpu.pprof > profileRun1.pdf Example CPU and Memory profile of a recent release.","title":"Profiling  (for dev work)"},{"location":"community/","text":"Users of Gleaner \uf0c1 The following are some communities using or exploring the use of Gleaner. \uf0c1 https://geocodes.earthcube.org/ GeoCODES is an NSF Earthcube program effort to better enable cross-domain discovery of and access to geoscience data and research tools. GeoCODES is made up of three components respectively. Ocean InfoHub \uf0c1 https://oceaninfohub.org/ The Ocean InfoHub (OIH) Project aims to improve access to global oceans information, data and knowledge products for management and sustainable development.The OIH will link and anchor a network of regional and thematic nodes that will improve online access to and synthesis of existing global, regional and national data, information and knowledge resources, including existing clearinghouse mechanisms. The project will not be establishing a new database, but will be supporting discovery and interoperability of existing information systems.The OIH Project is a three-year project funded by the Government of Flanders, Kingdom of Belgium, and implemented by the IODE Project Office of the IOC/UNESCO. Polder: Polar Data Discovery Enhancement Research \uf0c1 https://polder.info/ Federated metadata search for the polar regions will dramatically simplify data discovery for polar scientists. Instead of searching dozens of metadata catalogues separately, a user can come to a single search page. This is a rapidly moving field and POLDER is working to find the best path forward for our community. POLDER is a collaboration between the Southern Ocean Observing System, Arctic Data Committee, and Standing Committee on Antarctic Data Management. Canadian Consortium for Arctic Data Interoperability \uf0c1 https://ccadi.ca/ The Canadian Consortium for Arctic Data Interoperability (CCADI) is an initiative to develop an integrated Canadian arctic data management system that will facilitate information discovery, establish sharing standards, enable interoperability among existing data infrastructures, and that will be co-designed with, and accessible to, a broad user base. Key to the CCADI vision are: standards and mechanisms for metadata, data and semantic interoperability; a distributed data exchange platform; streamlined data services with common entry, access, search, match, analysis, visualization and output tools; an intellectual property and sensitive data service; and data stewardship capacity. Internet of Water (planned) \uf0c1 Geoconnex Geoconnex rests on widespread adoption of metadata best practices, automatically harvesting metadata and indexing data to real-world hydrologic features (e.g. lakes, reservoirs, wells, streams, water distribution systems, monitoring locations). The resulting water-specific search index will be browsable from a common water metadata catalog for the IoW network in both a human and machine-readable format.","title":"Users of Gleaner"},{"location":"community/#users-of-gleaner","text":"The following are some communities using or exploring the use of Gleaner.","title":"Users of Gleaner"},{"location":"community/#_1","text":"https://geocodes.earthcube.org/ GeoCODES is an NSF Earthcube program effort to better enable cross-domain discovery of and access to geoscience data and research tools. GeoCODES is made up of three components respectively.","title":""},{"location":"community/#ocean-infohub","text":"https://oceaninfohub.org/ The Ocean InfoHub (OIH) Project aims to improve access to global oceans information, data and knowledge products for management and sustainable development.The OIH will link and anchor a network of regional and thematic nodes that will improve online access to and synthesis of existing global, regional and national data, information and knowledge resources, including existing clearinghouse mechanisms. The project will not be establishing a new database, but will be supporting discovery and interoperability of existing information systems.The OIH Project is a three-year project funded by the Government of Flanders, Kingdom of Belgium, and implemented by the IODE Project Office of the IOC/UNESCO.","title":" Ocean InfoHub"},{"location":"community/#polder-polar-data-discovery-enhancement-research","text":"https://polder.info/ Federated metadata search for the polar regions will dramatically simplify data discovery for polar scientists. Instead of searching dozens of metadata catalogues separately, a user can come to a single search page. This is a rapidly moving field and POLDER is working to find the best path forward for our community. POLDER is a collaboration between the Southern Ocean Observing System, Arctic Data Committee, and Standing Committee on Antarctic Data Management.","title":"Polder: Polar Data Discovery Enhancement Research"},{"location":"community/#canadian-consortium-for-arctic-data-interoperability","text":"https://ccadi.ca/ The Canadian Consortium for Arctic Data Interoperability (CCADI) is an initiative to develop an integrated Canadian arctic data management system that will facilitate information discovery, establish sharing standards, enable interoperability among existing data infrastructures, and that will be co-designed with, and accessible to, a broad user base. Key to the CCADI vision are: standards and mechanisms for metadata, data and semantic interoperability; a distributed data exchange platform; streamlined data services with common entry, access, search, match, analysis, visualization and output tools; an intellectual property and sensitive data service; and data stewardship capacity.","title":"  Canadian Consortium for Arctic Data Interoperability"},{"location":"community/#internet-of-water-planned","text":"Geoconnex Geoconnex rests on widespread adoption of metadata best practices, automatically harvesting metadata and indexing data to real-world hydrologic features (e.g. lakes, reservoirs, wells, streams, water distribution systems, monitoring locations). The resulting water-specific search index will be browsable from a common water metadata catalog for the IoW network in both a human and machine-readable format.","title":" Internet of Water (planned)"},{"location":"glcon/","text":"glcon processing console for gleaner and nabu \uf0c1 glcon is used to generate configurations, run gleaner and nabu for those configurations. installation, see binary installation Running glcon configuration, see configs/template/README_Configure_Template.md Sources configuration: * Sitemap * Github * GoogleDrive","title":"Glcon"},{"location":"glcon/#glcon-processing-console-for-gleaner-and-nabu","text":"glcon is used to generate configurations, run gleaner and nabu for those configurations. installation, see binary installation Running glcon configuration, see configs/template/README_Configure_Template.md Sources configuration: * Sitemap * Github * GoogleDrive","title":"glcon processing console for gleaner and nabu"},{"location":"sitemaps/","text":"Sitemaps \uf0c1 About \uf0c1 Sitemaps are a means to inform indexing machines about the resources at a web server. Sitemaps can be in either TXT or XML format, with the XML being the more common. Information about sitemaps can be found at https://www.sitemaps.org. For resources you wish to have indexed by the Google Data Set Search engine or the EarthCube Gleaner code base or other tools, you should use a sitemap. While the TXT format is supported by all these tools, the XML format is highly recommended. The basic structure of the sitemap is: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"> <url> <loc>https://opencoredata.org/id/dataset/8c34c20f-34b7-47fd-b8a9-410ecd86a6b3</loc> </url> <url> <loc>https://opencoredata.org/id/dataset/84a22f9c-ac99-4adf-834e-3892fe28e660</loc> </url> </urlset> If your web site already has a sitemap, it is fine to add the URLs for your landing pages in that sitemap. Systems like Gleaner and of course Google and others will inspect pages for the desired JSON-LD data graph packages. Sitemap Index \uf0c1 You can also use a sitemap index which is in effect a sitemap of sitemaps. An index might look like: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"> <sitemap> <loc>http://www.example.com/sitemap1.xml</loc> <lastmod>2004-10-01T18:23:17+00:00</lastmod> </sitemap> <sitemap> <loc>http://www.example.com/sitemap2.xml</loc> <lastmod>2005-01-01</lastmod> </sitemap> </sitemapindex> In this case sitemap1.xml might be your information and general site pages. Then, sitemap2.xml could be dedicated to your data set landing pages. A sitemap can only have 50,000 entries, so if you have more than that you will also need to use a sitemap index to spread the entries across 50K or less chunks with the files being referenced in the index. Robots.txt \uf0c1 You can also list sitemaps in your robots.txt file and there are some interesting things you can do there as well to direct various agents to different results or sitemaps. Ref: * https://tools.ietf.org/html/draft-rep-wg-topic-00 * https://support.google.com/webmasters/answer/6062596?hl=en A basic robots.txt might look like: User-agent: * Disallow: Sitemap: https://opencoredata.org/sitemap.xml You could also specify instructions for certain agents such as googlebot or the EarthCube_DataBot/1.0 (Gleaner).","title":"Sitemaps"},{"location":"sitemaps/#sitemaps","text":"","title":"Sitemaps"},{"location":"sitemaps/#about","text":"Sitemaps are a means to inform indexing machines about the resources at a web server. Sitemaps can be in either TXT or XML format, with the XML being the more common. Information about sitemaps can be found at https://www.sitemaps.org. For resources you wish to have indexed by the Google Data Set Search engine or the EarthCube Gleaner code base or other tools, you should use a sitemap. While the TXT format is supported by all these tools, the XML format is highly recommended. The basic structure of the sitemap is: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"> <url> <loc>https://opencoredata.org/id/dataset/8c34c20f-34b7-47fd-b8a9-410ecd86a6b3</loc> </url> <url> <loc>https://opencoredata.org/id/dataset/84a22f9c-ac99-4adf-834e-3892fe28e660</loc> </url> </urlset> If your web site already has a sitemap, it is fine to add the URLs for your landing pages in that sitemap. Systems like Gleaner and of course Google and others will inspect pages for the desired JSON-LD data graph packages.","title":"About"},{"location":"sitemaps/#sitemap-index","text":"You can also use a sitemap index which is in effect a sitemap of sitemaps. An index might look like: <?xml version=\"1.0\" encoding=\"UTF-8\"?> <sitemapindex xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"> <sitemap> <loc>http://www.example.com/sitemap1.xml</loc> <lastmod>2004-10-01T18:23:17+00:00</lastmod> </sitemap> <sitemap> <loc>http://www.example.com/sitemap2.xml</loc> <lastmod>2005-01-01</lastmod> </sitemap> </sitemapindex> In this case sitemap1.xml might be your information and general site pages. Then, sitemap2.xml could be dedicated to your data set landing pages. A sitemap can only have 50,000 entries, so if you have more than that you will also need to use a sitemap index to spread the entries across 50K or less chunks with the files being referenced in the index.","title":"Sitemap Index"},{"location":"sitemaps/#robotstxt","text":"You can also list sitemaps in your robots.txt file and there are some interesting things you can do there as well to direct various agents to different results or sitemaps. Ref: * https://tools.ietf.org/html/draft-rep-wg-topic-00 * https://support.google.com/webmasters/answer/6062596?hl=en A basic robots.txt might look like: User-agent: * Disallow: Sitemap: https://opencoredata.org/sitemap.xml You could also specify instructions for certain agents such as googlebot or the EarthCube_DataBot/1.0 (Gleaner).","title":"Robots.txt"},{"location":"Deprecated/DEMO/","text":"ECAM Gleaner Demo \uf0c1 Prerequsits \uf0c1 The only real requirement is a running Docker installation. You can learn more about docker and how to install it for your system at https://www.docker.com/ Downloads \uf0c1 Pull down the docker config from https://github.com/gleanerio/gleaner/tree/master/deployments We will use the gleaner-base.yml Pull down the \"starterpack.zip\" from https://github.com/gleanerio/gleaner/releases Pull down the copy of gleaner for your system from the distribution page The minio S3 client mc is also needed. https://min.io/download You will likely need to \"chmod +x mc\" that client You can also see usage with: ./mc --help The command \"./mc config\" is used to generate your files. Locate them and edit them with the access and secret keys you used in the prod.env file Set up your environment \uf0c1 Make a directory to act at the data volume. This is where files and data are persisted between container runs. For this demo we will use a directory called DV located at /root/DV You will need to build an environment file. There is one called demo.env you can pull out of the starterpack.zip You can also copy this file to .env as well so that it is default read by docker compose These environment files should be in the same directory you will place your docker compose file. You will need a docker configuration file. There are a few (the ones ending .yml) in the starterpack.zip file. You can also find them at the GitHub repo for Gleaner in the deployments directory. Be sure to export these variables to the shell you are running gleaner from. Setup runs \uf0c1 You will need to run docker-compose -f config.yml up -d (or equivelant) to setup the docker containers needed by Gleaner. The staterpack.zip contains a basic compose file you can start with and modify to your needs. You need to run Gleaner with the flag to check and create the needed buckets ./gleaner -setup ./gleaner -address=localhost -port=9000 -setup You need to copy the shape graphs into the gleaner-shacl bucket to support the optional SHACL validation miller. mc cp *.ttl local/gleaner-shacl First run \uf0c1 With all the containers running and the propper buckets in place in minio we are ready to run Gleaner. We need to ensure our configuration file is ready to go. There are a couple included in the starterpack.zip you can use for some testing runs. gleaner -configfile ssdb.yml Docker notes \uf0c1 You will not really need to use these commands. However, if you want to clean up some of the docker containers, the following commands are useful. Careful, these commands are rather sweeping. kill all running containers with docker kill $(docker ps -q) delete all stopped containers with docker rm $(docker ps -a -q) delete all images with docker rmi $(docker images -q)","title":"ECAM Gleaner Demo"},{"location":"Deprecated/DEMO/#ecam-gleaner-demo","text":"","title":"ECAM Gleaner Demo"},{"location":"Deprecated/DEMO/#prerequsits","text":"The only real requirement is a running Docker installation. You can learn more about docker and how to install it for your system at https://www.docker.com/","title":"Prerequsits"},{"location":"Deprecated/DEMO/#downloads","text":"Pull down the docker config from https://github.com/gleanerio/gleaner/tree/master/deployments We will use the gleaner-base.yml Pull down the \"starterpack.zip\" from https://github.com/gleanerio/gleaner/releases Pull down the copy of gleaner for your system from the distribution page The minio S3 client mc is also needed. https://min.io/download You will likely need to \"chmod +x mc\" that client You can also see usage with: ./mc --help The command \"./mc config\" is used to generate your files. Locate them and edit them with the access and secret keys you used in the prod.env file","title":"Downloads"},{"location":"Deprecated/DEMO/#set-up-your-environment","text":"Make a directory to act at the data volume. This is where files and data are persisted between container runs. For this demo we will use a directory called DV located at /root/DV You will need to build an environment file. There is one called demo.env you can pull out of the starterpack.zip You can also copy this file to .env as well so that it is default read by docker compose These environment files should be in the same directory you will place your docker compose file. You will need a docker configuration file. There are a few (the ones ending .yml) in the starterpack.zip file. You can also find them at the GitHub repo for Gleaner in the deployments directory. Be sure to export these variables to the shell you are running gleaner from.","title":"Set up your environment"},{"location":"Deprecated/DEMO/#setup-runs","text":"You will need to run docker-compose -f config.yml up -d (or equivelant) to setup the docker containers needed by Gleaner. The staterpack.zip contains a basic compose file you can start with and modify to your needs. You need to run Gleaner with the flag to check and create the needed buckets ./gleaner -setup ./gleaner -address=localhost -port=9000 -setup You need to copy the shape graphs into the gleaner-shacl bucket to support the optional SHACL validation miller. mc cp *.ttl local/gleaner-shacl","title":"Setup runs"},{"location":"Deprecated/DEMO/#first-run","text":"With all the containers running and the propper buckets in place in minio we are ready to run Gleaner. We need to ensure our configuration file is ready to go. There are a couple included in the starterpack.zip you can use for some testing runs. gleaner -configfile ssdb.yml","title":"First run"},{"location":"Deprecated/DEMO/#docker-notes","text":"You will not really need to use these commands. However, if you want to clean up some of the docker containers, the following commands are useful. Careful, these commands are rather sweeping. kill all running containers with docker kill $(docker ps -q) delete all stopped containers with docker rm $(docker ps -a -q) delete all images with docker rmi $(docker images -q)","title":"Docker notes"},{"location":"Deprecated/runningGleanerOLD/","text":"Gleaner getting started \uf0c1 A video of this guide is available at: https://vimeo.com/372081748 This guide is intended to walk through getting started with Gleaner in the easier manner. It is not the only way to use this program and may not be the best way for your environment or use case. However, it will give a general overview and many of the points here are common across all the ways to use this program. First, what is Gleaner. Gleaner is a tool for extracting structured data on the web from a set of define providers. It is not a web crawler and will not follow links in the pages it access. It use a sitemap file created by a provider which is a set of URLs to resources Gleaner will visit. Gleaner then extracts the structured data from the web represented by JSON-LD. Some readers will note there are others ways, like RDFA, to represent this data on a web site. Gleaner only looks for JSON-LD at this time. If you are interested in publishing this sort of data, take a look at the ESIP hosted Science on Scheme GitHub repository and also the Google Developer Guide on publishing this sort of data as a provider. Prerequisites \uf0c1 Docker \uf0c1 To start this guide you will need a few things first. One is a computer with Docker installed. Docker is a popular tool for creating and using containers. Containers are packaged applications like databases, games or web servers. The Docker runtime providers a cross platform environment to run this common container images. Images are downloaded from the net and can be maintained and updated. Containers can be run in large cloud based environments with sophisticated orchestration systems or on your local computer. For this example we will be running on a rather simple Linux based server provided by NSF's XSEDE environment. However, any personal computer will do just fine. You can download Docekr your PC or Mac at https://www.docker.com/products/docker-desktop and Linux users can typically just use your distro's package management system. Once you have Docker installed and verified its operation you will need to download the Gleaner \"Stater Pack\". Reference the Gleaner releases at https://github.com/gleanerio/gleaner/releases to download the needed files for this guide. In particular the starterpack.zip located in the assets section of the releases. Gleaner is a command line application, so you will need a terminal program on your computer and be comfortable issuing basic commands. Testing on Windows has not taken place yet. This documentation will be updated when that is done. These scripts are just simple Docker commands, so use them as a guide and we will work to generate the Windows scripts ASAP. Note, when you first install Docker on a new system you will need to opt it into \"swarm\" mode to use the configuration files we will get to. To do this use docker swarm init You can always turn swarm mode off late if you wish to. Use: docker swarm leave Starter Pack \uf0c1 To provide you a set of files and scripts to bootstrap the process we have created a starter pack ZIP file. Let's make a directory and download the starter pack. Visit the web site at https://github.com/gleanerio/gleaner/releases and find the latest release. In the assets drop down section you will find links to the various assets of that release. We will need the starterpack.zip file. Here we will use wget to download the file, but you could use curl or just web browser to download this file. Be sure to visit the release page at GitHub and not just use the command from below as this will likely be out of date with the current release versions. root@gleaner:~# mkdir gleanerRuns root@gleaner:~# cd gleanerRuns/ root@gleaner:~/gleanerRuns# wget https://github.com/gleanerio/gleaner/releases/download/2.0.11/starterpack.zip [...] Saving to: \u2018starterpack.zip\u2019 [...] 2019-11-05 04:53:10 (41.8 MB/s) - \u2018starterpack.zip\u2019 saved [1752/1752] root@gleaner:~/gleanerRuns# unzip starterpack.zip Archive: starterpack.zip inflating: starterpack/demo.env inflating: starterpack/gleaner-base.yml creating: starterpack/shapefiles/ inflating: starterpack/v2config.yaml root@gleaner:~/gleanerRuns# cd starterpack/ root@gleaner:~/gleanerRuns/starterpack# ls -lt total 164 -rw-r--r-- 1 root root 356 Oct 29 10:25 README.md -rw-r--r-- 1 root root 1651 Oct 23 22:49 config.yaml -rw-r--r-- 1 root root 1042 Oct 23 22:45 docker-compose.yml -rwxr-xr-x 1 root root 320 Oct 23 22:42 runGleaner.sh -rw-r--r-- 1 root root 137 Oct 23 21:40 demo.env drwxr-xr-x 2 root root 4096 Oct 20 03:00 shapegraphs -rw-rw-r-- 1 root root 141567 Oct 8 11:52 jsonldcontext.json These files are the ones we will now start with. Setting the Environment Variables \uf0c1 There are a few things we need to do first to set out the environment in which Gleaner will run. One is actually set a few \"environment variables\". If you look at the demo.env file you will see: root@gleaner:~/gleanerRuns/starterpack# cat demo.env # Set environments export MINIO_ACCESS_KEY=\"MySecretAccessKey\" export MINIO_SECRET_KEY=\"MySecretSecretKeyforMinio\" export DATAVOL=\"./DV\" A note on these values. The Minio entries are for the Minio object storage system that Gleaner uses and that we will install and run here soon via Docker. You might wish to change these though note if you are just running Minio locally and for Gleaner only on a system not Internet accessible these are OK to start with. Good practice would be to change them of course. Data Volume \uf0c1 The DATAVOL variable will be used to define where Minio and other elements of the run store data. You don't have to use a DATAVOL mount but it is a good idea. First, it will persist your data should you shutdown and restart your Docker containers later, which you likely will. Second, it will give a performance nudge over writing to the Docker file system, which is an abstraction over your native file system. If you are more familiar with Docker, feel free to change this. Setting \uf0c1 We need to ensure these are set in our terminal (shell). For most of you should be able to simply source this file and set the values. In the following we will check for a variable, not find it, source the file and then confirm we see it. root@gleaner:~/gleanerRuns/starterpack# echo $DATAVOL root@gleaner:~/gleanerRuns/starterpack# source demo.env root@gleaner:~/gleanerRuns/starterpack# echo $DATAVOL ./DV Note that the file assumes BASH or a BASH compliant shell. If you are running ZSH or another shell, there are some other steps you will need to take to set these values. Those of you more familiar with Docker might note you could copy this file to a new name. Specifically a .env file located in the same directory as your docker-compose.yml file. This should also work. Also, the compose file actually reference this demo.env file as an \"env_file\" entry. However, I have seen cases where some of these approaches do not always work, so manually setting them and confirming them is a good move unless you are more familiar with Docker and Docker annoyances. Docker Compose command \uf0c1 We are not ready to set up the containers we need running to support Gleaner. In the starter pack there is a file called docker-compose.yml. It's a bit large for this document but you can view the version on GitHub at: https://github.com/gleanerio/gleaner/blob/master/docs/starterpack/docker-compose.yml This file contains the instructions Docker will use to download and run the various container images we need. If you look at the file you will see 6 images. Their labels and a short description follow; mc glenaer minio tangram headless jena Getting the images \uf0c1 The first thing we can do is download the images we will need. You do not need to do this separately, issueing the run command in Docker will check for and download any required images. For this document, however, let's do it as a special command. Note, you only need to download an image once, it will then been stored local to your system as an image and will run from there. Later you can check for new versions or updates too. If we look at our images we might not see anything if this is a new system with Docker. root@gleaner:~/gleanerRuns/staterpack# docker images REPOSITORY TAG IMAGE ID CREATED SIZE Let's download our images. Still in our starterpack directory we can use the command: root@gleaner:~/gleanerRuns/starterpack# docker-compose -f docker-compose.yml pull [... Docker will report out the progress here, I have removed it for brevity] root@gleaner:~/gleanerRuns/starterpack# docker images REPOSITORY TAG IMAGE ID CREATED SIZE chromedp/headless-shell latest 2c051a7d9f70 10 hours ago 220MB nsfearthcube/gleaner latest c5fb0023d473 2 weeks ago 104MB minio/minio latest 8869bca0366f 3 weeks ago 51MB minio/mc latest f4f9de663a7f 3 weeks ago 22.7MB fils/p418tangram 0.1.15 833aa7811eb1 3 weeks ago 991MB fcore/p418jena 0.1.11 879cafba0181 4 months ago 2.67GB Depending on your network, this might take a minute or two. After the pull command we can rerun the \"docker images\" command to now see the images we will be using for our run. We host these at https://hub.docker.com/ and there are over a 100,000 containers there from the Docker community. Run docker-compose \uf0c1 At this point we are ready to set up the environment. As noted, do this without doing the above pull command is fine. Docker will see what images it needs to satisfy a compose file and fetch them. We have: set up our environment variables downloaded our images Next we need to issue the command to run these images. We can see what containers we have already with the docker ps command as in: root@gleaner:~/gleanerRuns/starterpack# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Here we have no running containers. Let's run some. root@gleaner:~/gleanerRuns/starterpack# docker-compose -f docker-compose.yml up -d WARNING: The Docker Engine you're using is running in swarm mode. Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node. To deploy your application across the swarm, use `docker stack deploy`. Creating network \"starterpack_default\" with the default driver Creating network \"starterpack_web\" with driver \"overlay\" Creating starterpack_jena_1 ... Creating starterpack_gleaner_1 ... Creating starterpack_minio_1 ... Creating starterpack_tangram_1 ... Creating starterpack_headless_1 ... Creating starterpack_mc_1 ... Creating starterpack_gleaner_1 Creating starterpack_jena_1 Creating starterpack_minio_1 Creating starterpack_mc_1 Creating starterpack_headless_1 Creating starterpack_jena_1 ... done root@gleaner:~/gleanerRuns/starterpack# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 789f5343f06a fils/p418tangram:0.1.15 \"/bin/sh -c 'exec gu\u2026\" 12 seconds ago Up 6 seconds 0.0.0.0:8080->8080/tcp starterpack_tangram_1 dc22eb223210 chromedp/headless-shell:latest \"/headless-shell/hea\u2026\" 12 seconds ago Up 5 seconds 0.0.0.0:32772->9222/tcp starterpack_headless_1 7026b893f9b1 fcore/p418jena:0.1.11 \"/usr/local/bin/entr\u2026\" 12 seconds ago Up 4 seconds (health: starting) 0.0.0.0:3030->3030/tcp starterpack_jena_1 730263a80255 minio/mc:latest \"mc\" 12 seconds ago Exited (0) 9 seconds ago starterpack_mc_1 8beac8e063c2 minio/minio:latest \"/usr/bin/docker-ent\u2026\" 12 seconds ago Up 6 seconds 0.0.0.0:9000->9000/tcp starterpack_minio_1 408610b309b9 nsfearthcube/gleaner:latest \"/gleaner/gleaner\" 12 seconds ago Exited (2) 8 seconds ago starterpack_gleaner_1 root@gleaner:~/gleanerRuns/starterpack# A few things to note in the above: swarm \uf0c1 The WARNING about swarm you likely will NOT see as you likely are NOT running in Swarm mode. That is fine. If you are running in swarm mode (you likely don't need this document) but you might be using the command docker stack deploy --compose-file docker-compose.yml gleaner as some swarm instances will not have the docker-compose command. ports \uf0c1 You can see that our containers are using some ports to communicate on. If you are experienced with Docker and running other containers you may have an issue where ports are already in use. You will need to resolve that. Note, Gleaner goes looking for services on these ports and I don't currently offer the ability to change that. So you will need to resolve it. The required ports are: 8080, 32772, 3030 and 9000. In future releases I will try and note that these should be changed to less popular ports. 8080 in particular may be problematic if you are running other containers as it's a popular port for local http services. Gleaner Configuration \uf0c1 So now we are ready to review the Gleaner configuration file named config.yml. There is actually quite a bit in this file, but for this starting demo only a few things we need to worry about. The default file will look like: --- minio: address: localhost port: 9000 accesskey: MySecretAccessKey secretkey: MySecretSecretKeyforMinio ssl: false gleaner: runid: demo # this will be the bucket the output is placed in... summon: true # do we want to visit the web sites and pull down the files mill: true tmpdir: \"\" context: cache: true contextmaps: - prefix: \"https://schema.org/\" file: \"/gleaner/config/jsonldcontext.json\" - prefix: \"http://schema.org/\" file: \"/gleaner/config/jsonldcontext.json\" - prefix: \"https://schema.org\" file: \"/gleaner/config/jsonldcontext.json\" - prefix: \"http://schema.org\" file: \"/gleaner/config/jsonldcontext.json\" summoner: mode: diff # [time, hash, none] diff: look for difference or full: delete old objects and replace millers: graph: true shacl: false prov: false shapefiles: - ref: https://raw.githubusercontent.com/geoschemas-org/geoshapes/master/shapegraphs/googleRequired.ttl - ref: https://raw.githubusercontent.com/geoschemas-org/geoshapes/master/shapegraphs/googleRecommendedCoverageCheck.ttl sources: - name: opencore logo: http://geodex.org/images/logos/EarthCubeLogo.png url: http://opencoredata.org/sitemap.xml headless: false A few things we need to look at. First, in the \"mino:\" section make sure the accessKey and secretKey here match the ones you have and set via your demo.env file. Next, lets look at the \"gleaner:\" section. We can set the runid to something. This is the ID for a run and it allows you to later make different runs and keep the resulting graphs organized. It can be set to any lower case string with no spaces. The miller and summon sections are true and we will leave them that way. It means we want Gleaner to both fetch the resources and process (mill) them. Now look at the \"miller:\" section when lets of pick what milling to do. Currently it is set with only graph set to true. Let's leave it that way for now. This means Gleaner will only attempt to make graph and not also run validation or generate prov reports for the process. The final section we need to look at is the \"sources:\" section. Here is where the fun is. sources: - name: opencore logo: http://geodex.org/images/logos/EarthCubeLogo.png url: http://opencoredata.org/sitemap.xml headless: false These are the sources we wish to pull and process. Each source has 4 entries though at this time we no longer use the \"logo\" value. It was used in the past to provide a page showing all the sources and a logo for them. However, that's really just out of scope for what we want to do. You can leave it blank or set it to any value, it wont make a difference. The name is what you want to call this source. It should be one word (no space) and be lower case. The url value needs to point to the URL for the site map XML file. This will be created and served by the data provider. The headless value should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page. This is true of some sites and it is supported but not currently auto-detected. So you will need to know this and set it. For most place, this will be false. You can have as many sources as you wish. For an example look the configure file for the CDF Semantic Network at: https://github.com/gleanerio/CDFSemanticNetwork/blob/master/configs/cdf.yaml For this demo we will use the site map for Open Core data. However, I would prefer to get a better and smaller example source that could highlight various capabilities and edge cases for this package. A more detailed review of the configuration file will be made and linked here. Run Gleaner via Docker \uf0c1 With our configuration file ready we have arrived at the time when we can run Gleaner. First we need to make sure that we have Minio set up with the correct buckets. Use the runGleaner.sh script with the --setup flag. root@gleaner:~/gleanerRuns/starterpack# ./runGleaner.sh --setup my-vol main.go:37: EarthCube Gleaner main.go:92: Setting up buckets check.go:56: Gleaner Bucket gleaner not found, generating check.go:56: Gleaner Bucket gleaner-summoned not found, generating check.go:56: Gleaner Bucket gleaner-milled not found, generating main.go:98: Buckets generated. Object store should be ready for runs my-vol We only need to run this the first time. It will look for the Minio buckets (storage containers) we need and set them up if not found. It wont hurt anything to run this again, so if you think you have not run it, go ahead and issue the command. Likely this should just be an init call in the main run of Gleaner and then could be removed as a step. With our system set up, containers running, configuration file finished and the buckets created we are ready to run Gleaner. Simply use the command: root@gleaner:~/gleanerRuns/starterpack# ./runGleaner.sh my-vol main.go:37: EarthCube Gleaner main.go:103: Validating access to object store main.go:110: Validating access to needed buckets check.go:37: Verfied Gleaner bucket: gleaner. check.go:37: Verfied Gleaner bucket: gleaner-summoned. check.go:37: Verfied Gleaner bucket: gleaner-milled. summoner.go:16: Summoner start time: 2019-11-06 14:12:46.636899634 +0000 UTC m=+0.025597505 resources.go:35: Parsing sitemap: http://opencoredata.org/sitemap.xml sitemaps.go:49: Content type of sitemap reference is text/xml; charset=utf-8 opencore 55m14s [--------------------------------------------------------------------] 100% summoner.go:29: Summoner end time: 2019-11-06 15:08:01.708350533 +0000 UTC m=+3315.097048410 millers.go:28: Miller start time: 2019-11-06 15:08:01.708401893 +0000 UTC m=+3315.097099749 millers.go:41: Adding bucket to milling list: opencore graphkv.go:29: Queuing URLs for opencore graphkv.go:238: Created tempdir /opencore /tmp/opencore812448063 graphkv.go:54: opencore graphkv.go:121: Start pipe reader / writer sequence graphkv.go:121: Start pipe reader / writer sequence millers.go:71: Miller end time: 2019-11-06 15:32:25.559500706 +0000 UTC m=+4778.948198623 millers.go:72: Miller run time: 24.397518 my-vol This may take some time to run especially if you have added more resources or if those sites or your network connection is slow. Reviewing the output \uf0c1 Once Gleaner is done and if everything has gone OK the results will be written to Minio buckets. This is done to better support automated work flows with the data, but it doesn't lend itself to the easiest use by humans. To access the results you will need a S3 capable client or your web browser. Browser \uf0c1 The easiest way by far would be to access the Minio server via your web browser. If you are running the Gleaner setup local this should work fine. However, if you are running remotely you may or may not have access to the port 9000 that Minio is running on. If you do, simply point your browser at http://localhost;9000 (assuming the same, local, machine here). The login screen will look like: Use your keys to login and the interface will look like: mc \uf0c1 For interested in a command line option Minio offers a free cross platform client. For the client, go to https://min.io/download and download the mc client (not server) for your platform. Follow the instructions to download and ensure the binary is executable so it can be run. You can then configure your server to the client with a command template like; mc config host add <ALIAS> <YOUR-S3-ENDPOINT> <YOUR-ACCESS-KEY> <YOUR-SECRET-KEY> <API-SIGNATURE> For our set up, it might look like. mc config host add local http://localhost:9000 MySecretAccessKey MySecretSecretKeyforMinio You can then view the buckets, content and even copy the files from Minio to your local file system to work with. root@gleaner:~/gleanerRuns/starterpack# wget https://dl.min.io/client/mc/release/linux-amd64/mc [ ... download progress removed ...] root@gleaner:~/gleanerRuns/starterpack# chmod +x mc root@gleaner:~/gleanerRuns/starterpack# ./mc config host add local http://localhost:9000 MySecretAccessKey MySecretSecretKeyforMinio Added `local` successfully. root@gleaner:~/gleanerRuns/starterpack# ./mc ls local [2019-11-05 10:23:53 HST] 0B gleaner/ [2019-11-06 05:32:13 HST] 0B gleaner-milled/ [2019-11-06 04:12:51 HST] 0B gleaner-summoned/ root@gleaner:~/gleanerRuns/starterpack# ./mc ls local/gleaner-summoned [2019-11-06 05:47:37 HST] 0B opencore/ root@gleaner:~/gleanerRuns/starterpack# ./mc ls local/gleaner-milled [2019-11-06 05:47:44 HST] 0B demo/ root@gleaner:~/gleanerRuns/starterpack# ./mc ls local/gleaner-milled/demo [2019-11-06 05:32:13 HST] 0B opencore_BadTriples.nq [2019-11-06 05:32:24 HST] 566MiB opencore_GoodTriples.nq root@gleaner:~/gleanerRuns/starterpack# ./mc cp local/gleaner-milled/demo/opencore_GoodTriples.nq . ...opencore_GoodTriples.nq: 565.46 MiB / 565.46 MiB \u2503\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2503 100.00% 285.81 MiB/s 1s root@gleaner:~/gleanerRuns/starterpack# ls -lt opencore_GoodTriples.nq -rw-r--r-- 1 root root 592925612 Nov 6 05:48 opencore_GoodTriples.nq root@gleaner:~/gleanerRuns/starterpack# So now we would have the output from Gleaner in our directory. The \"Good triples\" and \"Bad triples\" names are part of Gleaner reporting. Any triples that Gleaner finds that are not properly formed according to the RDF spec get reported and routed to this file so providers can review the issues. In this case we had no bad triples so the file was empty. The \"Good triples\" file can then be used in any RDF or RDF compatible triple store to support query and analysis. Other tools \uf0c1 If you already use other tools to interact with S3 compiant object stores you can use them with minio as well. These would be things like s3tools (python) or CyberDuck (Mac and Windows). Since it's an S3 API, there are even ways you can mount Minio to your file system and view the buckets and simple folders on your machine. Load to a triple store and query \uf0c1 As part of the process of loading Gleaner container an example triple store is loaded. This is the Jena Fuseki triple store and we can load our graph into this and begin to make queries on it. This is outside the scope of Gleaner now, but we will review it briefly here at the end to give an idea of how Gleaner products could be used. If you visit http://localhost:3030 you should find Jena available. Click the \"add data\" button to arrive at the screen where you can upload your data. Here you can put in a named graph entry. In the example below we used the run ID we provided to Gleaner. Now use the \"select files\" button to select your file. Then click \"upload all\" to try and load the file. If everything is OK with the RDF graph, you should get a nice green bar and the RDF will be loaded. Now use the query tab to navigate to the query interface and try out a query. In this example, we already new what a resource URI was and used that in our search to see what data was associated with it. You can explore more about SPARQL, the graph query language, at https://www.w3.org/TR/sparql11-overview/. Using something like Jena and SPARQL you could build out APIs or interfaces like we did with the https://geodex.org site below. Conclusion \uf0c1 This concludes the Gleaner walk through. The goal was to go from nothing to a semantic network. Gleaner is still in development as are these documentations. Any issues, suggestions or edits you have are more than welcome and you are encouraged to use the GitHub issue system to provide them.","title":"Gleaner getting started"},{"location":"Deprecated/runningGleanerOLD/#gleaner-getting-started","text":"A video of this guide is available at: https://vimeo.com/372081748 This guide is intended to walk through getting started with Gleaner in the easier manner. It is not the only way to use this program and may not be the best way for your environment or use case. However, it will give a general overview and many of the points here are common across all the ways to use this program. First, what is Gleaner. Gleaner is a tool for extracting structured data on the web from a set of define providers. It is not a web crawler and will not follow links in the pages it access. It use a sitemap file created by a provider which is a set of URLs to resources Gleaner will visit. Gleaner then extracts the structured data from the web represented by JSON-LD. Some readers will note there are others ways, like RDFA, to represent this data on a web site. Gleaner only looks for JSON-LD at this time. If you are interested in publishing this sort of data, take a look at the ESIP hosted Science on Scheme GitHub repository and also the Google Developer Guide on publishing this sort of data as a provider.","title":"Gleaner getting started"},{"location":"Deprecated/runningGleanerOLD/#prerequisites","text":"","title":"Prerequisites"},{"location":"Deprecated/runningGleanerOLD/#docker","text":"To start this guide you will need a few things first. One is a computer with Docker installed. Docker is a popular tool for creating and using containers. Containers are packaged applications like databases, games or web servers. The Docker runtime providers a cross platform environment to run this common container images. Images are downloaded from the net and can be maintained and updated. Containers can be run in large cloud based environments with sophisticated orchestration systems or on your local computer. For this example we will be running on a rather simple Linux based server provided by NSF's XSEDE environment. However, any personal computer will do just fine. You can download Docekr your PC or Mac at https://www.docker.com/products/docker-desktop and Linux users can typically just use your distro's package management system. Once you have Docker installed and verified its operation you will need to download the Gleaner \"Stater Pack\". Reference the Gleaner releases at https://github.com/gleanerio/gleaner/releases to download the needed files for this guide. In particular the starterpack.zip located in the assets section of the releases. Gleaner is a command line application, so you will need a terminal program on your computer and be comfortable issuing basic commands. Testing on Windows has not taken place yet. This documentation will be updated when that is done. These scripts are just simple Docker commands, so use them as a guide and we will work to generate the Windows scripts ASAP. Note, when you first install Docker on a new system you will need to opt it into \"swarm\" mode to use the configuration files we will get to. To do this use docker swarm init You can always turn swarm mode off late if you wish to. Use: docker swarm leave","title":"Docker"},{"location":"Deprecated/runningGleanerOLD/#starter-pack","text":"To provide you a set of files and scripts to bootstrap the process we have created a starter pack ZIP file. Let's make a directory and download the starter pack. Visit the web site at https://github.com/gleanerio/gleaner/releases and find the latest release. In the assets drop down section you will find links to the various assets of that release. We will need the starterpack.zip file. Here we will use wget to download the file, but you could use curl or just web browser to download this file. Be sure to visit the release page at GitHub and not just use the command from below as this will likely be out of date with the current release versions. root@gleaner:~# mkdir gleanerRuns root@gleaner:~# cd gleanerRuns/ root@gleaner:~/gleanerRuns# wget https://github.com/gleanerio/gleaner/releases/download/2.0.11/starterpack.zip [...] Saving to: \u2018starterpack.zip\u2019 [...] 2019-11-05 04:53:10 (41.8 MB/s) - \u2018starterpack.zip\u2019 saved [1752/1752] root@gleaner:~/gleanerRuns# unzip starterpack.zip Archive: starterpack.zip inflating: starterpack/demo.env inflating: starterpack/gleaner-base.yml creating: starterpack/shapefiles/ inflating: starterpack/v2config.yaml root@gleaner:~/gleanerRuns# cd starterpack/ root@gleaner:~/gleanerRuns/starterpack# ls -lt total 164 -rw-r--r-- 1 root root 356 Oct 29 10:25 README.md -rw-r--r-- 1 root root 1651 Oct 23 22:49 config.yaml -rw-r--r-- 1 root root 1042 Oct 23 22:45 docker-compose.yml -rwxr-xr-x 1 root root 320 Oct 23 22:42 runGleaner.sh -rw-r--r-- 1 root root 137 Oct 23 21:40 demo.env drwxr-xr-x 2 root root 4096 Oct 20 03:00 shapegraphs -rw-rw-r-- 1 root root 141567 Oct 8 11:52 jsonldcontext.json These files are the ones we will now start with.","title":"Starter Pack"},{"location":"Deprecated/runningGleanerOLD/#setting-the-environment-variables","text":"There are a few things we need to do first to set out the environment in which Gleaner will run. One is actually set a few \"environment variables\". If you look at the demo.env file you will see: root@gleaner:~/gleanerRuns/starterpack# cat demo.env # Set environments export MINIO_ACCESS_KEY=\"MySecretAccessKey\" export MINIO_SECRET_KEY=\"MySecretSecretKeyforMinio\" export DATAVOL=\"./DV\" A note on these values. The Minio entries are for the Minio object storage system that Gleaner uses and that we will install and run here soon via Docker. You might wish to change these though note if you are just running Minio locally and for Gleaner only on a system not Internet accessible these are OK to start with. Good practice would be to change them of course.","title":"Setting the Environment Variables"},{"location":"Deprecated/runningGleanerOLD/#data-volume","text":"The DATAVOL variable will be used to define where Minio and other elements of the run store data. You don't have to use a DATAVOL mount but it is a good idea. First, it will persist your data should you shutdown and restart your Docker containers later, which you likely will. Second, it will give a performance nudge over writing to the Docker file system, which is an abstraction over your native file system. If you are more familiar with Docker, feel free to change this.","title":"Data Volume"},{"location":"Deprecated/runningGleanerOLD/#setting","text":"We need to ensure these are set in our terminal (shell). For most of you should be able to simply source this file and set the values. In the following we will check for a variable, not find it, source the file and then confirm we see it. root@gleaner:~/gleanerRuns/starterpack# echo $DATAVOL root@gleaner:~/gleanerRuns/starterpack# source demo.env root@gleaner:~/gleanerRuns/starterpack# echo $DATAVOL ./DV Note that the file assumes BASH or a BASH compliant shell. If you are running ZSH or another shell, there are some other steps you will need to take to set these values. Those of you more familiar with Docker might note you could copy this file to a new name. Specifically a .env file located in the same directory as your docker-compose.yml file. This should also work. Also, the compose file actually reference this demo.env file as an \"env_file\" entry. However, I have seen cases where some of these approaches do not always work, so manually setting them and confirming them is a good move unless you are more familiar with Docker and Docker annoyances.","title":"Setting"},{"location":"Deprecated/runningGleanerOLD/#docker-compose-command","text":"We are not ready to set up the containers we need running to support Gleaner. In the starter pack there is a file called docker-compose.yml. It's a bit large for this document but you can view the version on GitHub at: https://github.com/gleanerio/gleaner/blob/master/docs/starterpack/docker-compose.yml This file contains the instructions Docker will use to download and run the various container images we need. If you look at the file you will see 6 images. Their labels and a short description follow; mc glenaer minio tangram headless jena","title":"Docker Compose command"},{"location":"Deprecated/runningGleanerOLD/#getting-the-images","text":"The first thing we can do is download the images we will need. You do not need to do this separately, issueing the run command in Docker will check for and download any required images. For this document, however, let's do it as a special command. Note, you only need to download an image once, it will then been stored local to your system as an image and will run from there. Later you can check for new versions or updates too. If we look at our images we might not see anything if this is a new system with Docker. root@gleaner:~/gleanerRuns/staterpack# docker images REPOSITORY TAG IMAGE ID CREATED SIZE Let's download our images. Still in our starterpack directory we can use the command: root@gleaner:~/gleanerRuns/starterpack# docker-compose -f docker-compose.yml pull [... Docker will report out the progress here, I have removed it for brevity] root@gleaner:~/gleanerRuns/starterpack# docker images REPOSITORY TAG IMAGE ID CREATED SIZE chromedp/headless-shell latest 2c051a7d9f70 10 hours ago 220MB nsfearthcube/gleaner latest c5fb0023d473 2 weeks ago 104MB minio/minio latest 8869bca0366f 3 weeks ago 51MB minio/mc latest f4f9de663a7f 3 weeks ago 22.7MB fils/p418tangram 0.1.15 833aa7811eb1 3 weeks ago 991MB fcore/p418jena 0.1.11 879cafba0181 4 months ago 2.67GB Depending on your network, this might take a minute or two. After the pull command we can rerun the \"docker images\" command to now see the images we will be using for our run. We host these at https://hub.docker.com/ and there are over a 100,000 containers there from the Docker community.","title":"Getting the images"},{"location":"Deprecated/runningGleanerOLD/#run-docker-compose","text":"At this point we are ready to set up the environment. As noted, do this without doing the above pull command is fine. Docker will see what images it needs to satisfy a compose file and fetch them. We have: set up our environment variables downloaded our images Next we need to issue the command to run these images. We can see what containers we have already with the docker ps command as in: root@gleaner:~/gleanerRuns/starterpack# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Here we have no running containers. Let's run some. root@gleaner:~/gleanerRuns/starterpack# docker-compose -f docker-compose.yml up -d WARNING: The Docker Engine you're using is running in swarm mode. Compose does not use swarm mode to deploy services to multiple nodes in a swarm. All containers will be scheduled on the current node. To deploy your application across the swarm, use `docker stack deploy`. Creating network \"starterpack_default\" with the default driver Creating network \"starterpack_web\" with driver \"overlay\" Creating starterpack_jena_1 ... Creating starterpack_gleaner_1 ... Creating starterpack_minio_1 ... Creating starterpack_tangram_1 ... Creating starterpack_headless_1 ... Creating starterpack_mc_1 ... Creating starterpack_gleaner_1 Creating starterpack_jena_1 Creating starterpack_minio_1 Creating starterpack_mc_1 Creating starterpack_headless_1 Creating starterpack_jena_1 ... done root@gleaner:~/gleanerRuns/starterpack# docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 789f5343f06a fils/p418tangram:0.1.15 \"/bin/sh -c 'exec gu\u2026\" 12 seconds ago Up 6 seconds 0.0.0.0:8080->8080/tcp starterpack_tangram_1 dc22eb223210 chromedp/headless-shell:latest \"/headless-shell/hea\u2026\" 12 seconds ago Up 5 seconds 0.0.0.0:32772->9222/tcp starterpack_headless_1 7026b893f9b1 fcore/p418jena:0.1.11 \"/usr/local/bin/entr\u2026\" 12 seconds ago Up 4 seconds (health: starting) 0.0.0.0:3030->3030/tcp starterpack_jena_1 730263a80255 minio/mc:latest \"mc\" 12 seconds ago Exited (0) 9 seconds ago starterpack_mc_1 8beac8e063c2 minio/minio:latest \"/usr/bin/docker-ent\u2026\" 12 seconds ago Up 6 seconds 0.0.0.0:9000->9000/tcp starterpack_minio_1 408610b309b9 nsfearthcube/gleaner:latest \"/gleaner/gleaner\" 12 seconds ago Exited (2) 8 seconds ago starterpack_gleaner_1 root@gleaner:~/gleanerRuns/starterpack# A few things to note in the above:","title":"Run docker-compose"},{"location":"Deprecated/runningGleanerOLD/#swarm","text":"The WARNING about swarm you likely will NOT see as you likely are NOT running in Swarm mode. That is fine. If you are running in swarm mode (you likely don't need this document) but you might be using the command docker stack deploy --compose-file docker-compose.yml gleaner as some swarm instances will not have the docker-compose command.","title":"swarm"},{"location":"Deprecated/runningGleanerOLD/#ports","text":"You can see that our containers are using some ports to communicate on. If you are experienced with Docker and running other containers you may have an issue where ports are already in use. You will need to resolve that. Note, Gleaner goes looking for services on these ports and I don't currently offer the ability to change that. So you will need to resolve it. The required ports are: 8080, 32772, 3030 and 9000. In future releases I will try and note that these should be changed to less popular ports. 8080 in particular may be problematic if you are running other containers as it's a popular port for local http services.","title":"ports"},{"location":"Deprecated/runningGleanerOLD/#gleaner-configuration","text":"So now we are ready to review the Gleaner configuration file named config.yml. There is actually quite a bit in this file, but for this starting demo only a few things we need to worry about. The default file will look like: --- minio: address: localhost port: 9000 accesskey: MySecretAccessKey secretkey: MySecretSecretKeyforMinio ssl: false gleaner: runid: demo # this will be the bucket the output is placed in... summon: true # do we want to visit the web sites and pull down the files mill: true tmpdir: \"\" context: cache: true contextmaps: - prefix: \"https://schema.org/\" file: \"/gleaner/config/jsonldcontext.json\" - prefix: \"http://schema.org/\" file: \"/gleaner/config/jsonldcontext.json\" - prefix: \"https://schema.org\" file: \"/gleaner/config/jsonldcontext.json\" - prefix: \"http://schema.org\" file: \"/gleaner/config/jsonldcontext.json\" summoner: mode: diff # [time, hash, none] diff: look for difference or full: delete old objects and replace millers: graph: true shacl: false prov: false shapefiles: - ref: https://raw.githubusercontent.com/geoschemas-org/geoshapes/master/shapegraphs/googleRequired.ttl - ref: https://raw.githubusercontent.com/geoschemas-org/geoshapes/master/shapegraphs/googleRecommendedCoverageCheck.ttl sources: - name: opencore logo: http://geodex.org/images/logos/EarthCubeLogo.png url: http://opencoredata.org/sitemap.xml headless: false A few things we need to look at. First, in the \"mino:\" section make sure the accessKey and secretKey here match the ones you have and set via your demo.env file. Next, lets look at the \"gleaner:\" section. We can set the runid to something. This is the ID for a run and it allows you to later make different runs and keep the resulting graphs organized. It can be set to any lower case string with no spaces. The miller and summon sections are true and we will leave them that way. It means we want Gleaner to both fetch the resources and process (mill) them. Now look at the \"miller:\" section when lets of pick what milling to do. Currently it is set with only graph set to true. Let's leave it that way for now. This means Gleaner will only attempt to make graph and not also run validation or generate prov reports for the process. The final section we need to look at is the \"sources:\" section. Here is where the fun is. sources: - name: opencore logo: http://geodex.org/images/logos/EarthCubeLogo.png url: http://opencoredata.org/sitemap.xml headless: false These are the sources we wish to pull and process. Each source has 4 entries though at this time we no longer use the \"logo\" value. It was used in the past to provide a page showing all the sources and a logo for them. However, that's really just out of scope for what we want to do. You can leave it blank or set it to any value, it wont make a difference. The name is what you want to call this source. It should be one word (no space) and be lower case. The url value needs to point to the URL for the site map XML file. This will be created and served by the data provider. The headless value should be set to false unless you know this site uses JavaScript to place the JSON-LD into the page. This is true of some sites and it is supported but not currently auto-detected. So you will need to know this and set it. For most place, this will be false. You can have as many sources as you wish. For an example look the configure file for the CDF Semantic Network at: https://github.com/gleanerio/CDFSemanticNetwork/blob/master/configs/cdf.yaml For this demo we will use the site map for Open Core data. However, I would prefer to get a better and smaller example source that could highlight various capabilities and edge cases for this package. A more detailed review of the configuration file will be made and linked here.","title":"Gleaner Configuration"},{"location":"Deprecated/runningGleanerOLD/#run-gleaner-via-docker","text":"With our configuration file ready we have arrived at the time when we can run Gleaner. First we need to make sure that we have Minio set up with the correct buckets. Use the runGleaner.sh script with the --setup flag. root@gleaner:~/gleanerRuns/starterpack# ./runGleaner.sh --setup my-vol main.go:37: EarthCube Gleaner main.go:92: Setting up buckets check.go:56: Gleaner Bucket gleaner not found, generating check.go:56: Gleaner Bucket gleaner-summoned not found, generating check.go:56: Gleaner Bucket gleaner-milled not found, generating main.go:98: Buckets generated. Object store should be ready for runs my-vol We only need to run this the first time. It will look for the Minio buckets (storage containers) we need and set them up if not found. It wont hurt anything to run this again, so if you think you have not run it, go ahead and issue the command. Likely this should just be an init call in the main run of Gleaner and then could be removed as a step. With our system set up, containers running, configuration file finished and the buckets created we are ready to run Gleaner. Simply use the command: root@gleaner:~/gleanerRuns/starterpack# ./runGleaner.sh my-vol main.go:37: EarthCube Gleaner main.go:103: Validating access to object store main.go:110: Validating access to needed buckets check.go:37: Verfied Gleaner bucket: gleaner. check.go:37: Verfied Gleaner bucket: gleaner-summoned. check.go:37: Verfied Gleaner bucket: gleaner-milled. summoner.go:16: Summoner start time: 2019-11-06 14:12:46.636899634 +0000 UTC m=+0.025597505 resources.go:35: Parsing sitemap: http://opencoredata.org/sitemap.xml sitemaps.go:49: Content type of sitemap reference is text/xml; charset=utf-8 opencore 55m14s [--------------------------------------------------------------------] 100% summoner.go:29: Summoner end time: 2019-11-06 15:08:01.708350533 +0000 UTC m=+3315.097048410 millers.go:28: Miller start time: 2019-11-06 15:08:01.708401893 +0000 UTC m=+3315.097099749 millers.go:41: Adding bucket to milling list: opencore graphkv.go:29: Queuing URLs for opencore graphkv.go:238: Created tempdir /opencore /tmp/opencore812448063 graphkv.go:54: opencore graphkv.go:121: Start pipe reader / writer sequence graphkv.go:121: Start pipe reader / writer sequence millers.go:71: Miller end time: 2019-11-06 15:32:25.559500706 +0000 UTC m=+4778.948198623 millers.go:72: Miller run time: 24.397518 my-vol This may take some time to run especially if you have added more resources or if those sites or your network connection is slow.","title":"Run Gleaner via Docker"},{"location":"Deprecated/runningGleanerOLD/#reviewing-the-output","text":"Once Gleaner is done and if everything has gone OK the results will be written to Minio buckets. This is done to better support automated work flows with the data, but it doesn't lend itself to the easiest use by humans. To access the results you will need a S3 capable client or your web browser.","title":"Reviewing the output"},{"location":"Deprecated/runningGleanerOLD/#browser","text":"The easiest way by far would be to access the Minio server via your web browser. If you are running the Gleaner setup local this should work fine. However, if you are running remotely you may or may not have access to the port 9000 that Minio is running on. If you do, simply point your browser at http://localhost;9000 (assuming the same, local, machine here). The login screen will look like: Use your keys to login and the interface will look like:","title":"Browser"},{"location":"Deprecated/runningGleanerOLD/#mc","text":"For interested in a command line option Minio offers a free cross platform client. For the client, go to https://min.io/download and download the mc client (not server) for your platform. Follow the instructions to download and ensure the binary is executable so it can be run. You can then configure your server to the client with a command template like; mc config host add <ALIAS> <YOUR-S3-ENDPOINT> <YOUR-ACCESS-KEY> <YOUR-SECRET-KEY> <API-SIGNATURE> For our set up, it might look like. mc config host add local http://localhost:9000 MySecretAccessKey MySecretSecretKeyforMinio You can then view the buckets, content and even copy the files from Minio to your local file system to work with. root@gleaner:~/gleanerRuns/starterpack# wget https://dl.min.io/client/mc/release/linux-amd64/mc [ ... download progress removed ...] root@gleaner:~/gleanerRuns/starterpack# chmod +x mc root@gleaner:~/gleanerRuns/starterpack# ./mc config host add local http://localhost:9000 MySecretAccessKey MySecretSecretKeyforMinio Added `local` successfully. root@gleaner:~/gleanerRuns/starterpack# ./mc ls local [2019-11-05 10:23:53 HST] 0B gleaner/ [2019-11-06 05:32:13 HST] 0B gleaner-milled/ [2019-11-06 04:12:51 HST] 0B gleaner-summoned/ root@gleaner:~/gleanerRuns/starterpack# ./mc ls local/gleaner-summoned [2019-11-06 05:47:37 HST] 0B opencore/ root@gleaner:~/gleanerRuns/starterpack# ./mc ls local/gleaner-milled [2019-11-06 05:47:44 HST] 0B demo/ root@gleaner:~/gleanerRuns/starterpack# ./mc ls local/gleaner-milled/demo [2019-11-06 05:32:13 HST] 0B opencore_BadTriples.nq [2019-11-06 05:32:24 HST] 566MiB opencore_GoodTriples.nq root@gleaner:~/gleanerRuns/starterpack# ./mc cp local/gleaner-milled/demo/opencore_GoodTriples.nq . ...opencore_GoodTriples.nq: 565.46 MiB / 565.46 MiB \u2503\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2593\u2503 100.00% 285.81 MiB/s 1s root@gleaner:~/gleanerRuns/starterpack# ls -lt opencore_GoodTriples.nq -rw-r--r-- 1 root root 592925612 Nov 6 05:48 opencore_GoodTriples.nq root@gleaner:~/gleanerRuns/starterpack# So now we would have the output from Gleaner in our directory. The \"Good triples\" and \"Bad triples\" names are part of Gleaner reporting. Any triples that Gleaner finds that are not properly formed according to the RDF spec get reported and routed to this file so providers can review the issues. In this case we had no bad triples so the file was empty. The \"Good triples\" file can then be used in any RDF or RDF compatible triple store to support query and analysis.","title":"mc"},{"location":"Deprecated/runningGleanerOLD/#other-tools","text":"If you already use other tools to interact with S3 compiant object stores you can use them with minio as well. These would be things like s3tools (python) or CyberDuck (Mac and Windows). Since it's an S3 API, there are even ways you can mount Minio to your file system and view the buckets and simple folders on your machine.","title":"Other tools"},{"location":"Deprecated/runningGleanerOLD/#load-to-a-triple-store-and-query","text":"As part of the process of loading Gleaner container an example triple store is loaded. This is the Jena Fuseki triple store and we can load our graph into this and begin to make queries on it. This is outside the scope of Gleaner now, but we will review it briefly here at the end to give an idea of how Gleaner products could be used. If you visit http://localhost:3030 you should find Jena available. Click the \"add data\" button to arrive at the screen where you can upload your data. Here you can put in a named graph entry. In the example below we used the run ID we provided to Gleaner. Now use the \"select files\" button to select your file. Then click \"upload all\" to try and load the file. If everything is OK with the RDF graph, you should get a nice green bar and the RDF will be loaded. Now use the query tab to navigate to the query interface and try out a query. In this example, we already new what a resource URI was and used that in our search to see what data was associated with it. You can explore more about SPARQL, the graph query language, at https://www.w3.org/TR/sparql11-overview/. Using something like Jena and SPARQL you could build out APIs or interfaces like we did with the https://geodex.org site below.","title":"Load to a triple store and query"},{"location":"Deprecated/runningGleanerOLD/#conclusion","text":"This concludes the Gleaner walk through. The goal was to go from nothing to a semantic network. Gleaner is still in development as are these documentations. Any issues, suggestions or edits you have are more than welcome and you are encouraged to use the GitHub issue system to provide them.","title":"Conclusion"},{"location":"cliDocker/","text":"Gleaner CLI Docker Image \uf0c1 About \uf0c1 This is a new approach for quick starts with Gleaner. It is a script that exposes a containerized version of Gleaner as a CLI interface. You can use the -init flag to pull down all the support files you need including the Docker Compose file for setting up the object store, a triplestore and the support for headless indexing. Prerequisites \uf0c1 You need Docker installed. Later, to work with the results and load them into a triplestore, you will also need an S3 compatible client. We will use the Minio client, mc, for this. Steps \uf0c1 Download the script gleanerDocker.sh from https://github.com/gleanerio/gleaner/tree/master/docs/cliDocker You may need to make it run-able with curl -O https://raw.githubusercontent.com/earthcubearchitecture-project418/gleaner/master/docs/cliDocker/gleanerDocker.sh chmod 755 gleanerDocker.sh Next you can run the script with the -init flag to pull down all the support files you need. ./gleanerDocker.sh -init This will also download the needed docker image and the support files. Your directory should look like this now: fils@ubuntu:~/clidocker# ls -lt total 1356 -rw-r--r-- 1 fils fils 1281 Aug 15 14:07 gleaner-IS.yml -rw-r--r-- 1 fils fils 290 Aug 15 14:07 setenvIS.sh -rw-r--r-- 1 fils fils 1266 Aug 15 14:07 demo.yaml -rw-r--r-- 1 fils fils 1371350 Aug 15 14:07 schemaorg-current-https.jsonld -rwxr-xr-x 1 fils fils 1852 Aug 15 14:06 gleanerDocker.sh Let's see if we can setup our support infrastructure for Gleaner. The file gleaner-IS.yml is a docker compose file that will set up the object store, and a triplestore. To do this we need to set up a few environment variables. To do this we can leverage the setenvIS.sh script. This script will set up the environment we need. Note you can also use a .env file or other approaches. You can references the Environment variables in Compose documentation. root@ubuntu:~/clidocker# source setenvIS.sh root@ubuntu:~/clidocker# docker-compose -f gleaner-IS.yml up -d Creating network \"clidocker_traefik_default\" with the default driver Creating clidocker_triplestore_1 ... done Creating clidocker_s3system_1 ... done Creating clidocker_headless_1 ... done Note: In a fresh run all the images will be pulled down. This may take a while. In the end, you should be able to see these images running: root@ubuntu:~/clidocker# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a26f7c945479 nawer/blazegraph \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute 0.0.0.0:9999->9999/tcp clidocker_triplestore_1 f3a4197c42be minio/minio:latest \"/usr/bin/docker-ent\u2026\" About a minute ago Up About a minute 0.0.0.0:9000->9000/tcp, 0.0.0.0:54321->54321/tcp clidocker_s3system_1 062f029462b1 chromedp/headless-shell:latest \"/headless-shell/hea\u2026\" About a minute ago Up About a minute 0.0.0.0:9222->9222/tcp At this point we should be able to do a run. During the init process a working config file was downloaded. Note: This config file will change... it's pointing to an OIH partner and I will not do that for the release. I have a demo site I will use. Next we need to setup our object for Gleaner. Gleaner itself can do this task so we will use root@ubuntu:~/clidocker# ./gleanerDocker.sh -setup -cfg demo main.go:35: EarthCube Gleaner main.go:110: Setting up buckets check.go:58: Gleaner Bucket gleaner not found, generating main.go:117: Buckets generated. Object store should be ready for runs Note: Here is where we go off the rails. The config file uses 0.0.0.0 as the location and this is not working. You need to edit the config file with the \"real\" IP of the host machine. In may case is this 192.168.122.77. This is obviously still a local network IP but it does work. I am still investigating this issue. We can now do a run with the example template file. Note: Best to delete the \"sitegraph\" node, I will do that soon. It should work, but is currently slow and gives little feedback If everything goes well, you should see something like the following: root@ubuntu:~/clidocker# ./gleanerDocker.sh -cfg demo main.go:35: EarthCube Gleaner main.go:122: Validating access to object store check.go:39: Validated access to object store: gleaner. org.go:156: Building organization graph (nq) org.go:163: {samplesearth https://samples.earth/sitemap.xml false https://www.re3data.org/repository/samplesearth Samples Earth (DEMO Site) https://samples.earth} main.go:154: Sitegraph(s) processed summoner.go:16: Summoner start time: 2021-08-15 14:34:08.907152656 +0000 UTC m=+0.067250623 resources.go:74: samplesearth : 202 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (202/202, 20 it/s) summoner.go:34: Summoner end time: 2021-08-15 14:34:20.36804137 +0000 UTC m=+11.528139340 summoner.go:35: Summoner run time: 0.191015 webfeed.go:37: 1758 millers.go:26: Miller start time: 2021-08-15 14:34:20.368063453 +0000 UTC m=+11.528161421 millers.go:40: Adding bucket to milling list: summoned/samplesearth millers.go:51: Adding bucket to prov building list: prov/samplesearth 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (202/202, 236 it/s) graphng.go:77: Assembling result graph for prefix: summoned/samplesearth to: milled/samplesearth graphng.go:78: Result graph will be at: results/runX/samplesearth_graph.nq pipecopy.go:16: Start pipe reader / writer sequence graphng.go:84: Pipe copy for graph done millers.go:80: Miller end time: 2021-08-15 14:34:21.84702814 +0000 UTC m=+13.007126109 millers.go:81: Miller run time: 0.024649 Working with results \uf0c1 If all has gone well, at this point you have downloaded the JSON-LD documents into Minio or some other object store.Next we will install a client that we can use to work with these objects. Note, there is a web interface exposed on the port mapped in the Docker compose file. In the case of these demo that is 9000. You can access it at http://localhost:9000/ with the credentials set in the environment variable file. However, to work with these objects it would be better to use a command line tool, like mc. The Minio Client, can be installed following their Minio Client Quickstate Guide . Be sure to place it somewhere where it can be seen from the command line, ie, make sure it is in your PATH variable. If you are on Linux this might look something like: wget https://dl.min.io/client/mc/release/linux-amd64/mc chmod +x mc ./mc --help There is also a Minio Client Docker image that you can use as well but it will be more difficult to use with the following scripts due to container isolation. To man an entry in the mc config use: mc alias set oih http://localhost:9000 worldsbestaccesskey worldsbestsecretkey We should now be able to list our object store. We have set it up using the alias oih . user@ubuntu:~/clidocker# mc ls oih [2021-08-15 14:31:20 UTC] 0B gleaner/ user@ubuntu:~/clidocker# mc ls oih/gleaner [2021-08-19 13:36:04 UTC] 0B milled/ [2021-08-19 13:36:04 UTC] 0B orgs/ [2021-08-19 13:36:04 UTC] 0B prov/ [2021-08-19 13:36:04 UTC] 0B results/ [2021-08-19 13:36:04 UTC] 0B summoned/ You can explore mc and see how to copy and work with the object store. Loading to the triplestore \uf0c1 As part of our Docker compose file we also spun up a triplestore. Let's use that now. Now Download the minio2blaze.sh script. curl -O https://raw.githubusercontent.com/earthcubearchitecture-project418/gleaner/master/scripts/minio2blaze.sh chmod 755 minio2blaze.sh The content we need to load into the triplestore needs to be in RDF for Blazegraph. We also need to tell the triplestore how we have encoded that RDF. If look in the object store at mc ls oih/gleaner/milled [2021-08-19 13:26:52 UTC] 0B samplesearth/ We should see a bucket that is holding the RDF data converted from the JSON-LD. Let's use this in our test. We can pass this path to the minio2blaze.sh script. This script will go looking for the mc command we installed above, so be sure it is in a PATH location that script can see. ./minio2blaze.sh oih/gleaner/milled/samplesearth ... lots of results removed If all has gone well, we should have RDF in the triplestore. We started our triplestore as part of the docker-compose.yml file. You can visit the triplestore at http://localhost:9999/blazegraph/#splash Note, you may have to try other addresses other than localhost if your networking is a bit different with Docker. For me, I had to use a real local IP address for my network, you might also try 0.0.0.0 . Hopefully you will see something like the following. We loaded into the default kb namespace, so we should be good there. We can see that is listed as the active namespace at the Current Namespace: kb report. Let's try a simple SPARQL query. Click on the Query tab to get to the query user interfaced. We can use something like: select * where { ?s ?p ?o } LIMIT 10 A very simple SPARQL to give us the first 10 results from the triplestore. If all has gone well, we should see something like: You can explore more about SPARQL and the wide range of queries you can do with it at the W3C SPARQL 1.1 Query Language reference. Conclusion \uf0c1 We have attempted here to give a quick introduction to the use of Gleaner in a Docker environment. This is a very simple example, but it should give you an idea of the approach used. This approach can then be combined with other approaches documented to establish a more production oriented implementation. Most of this documentation will be located at the Gleaner.io GitHub repository and Gleaner repository. Note: The plan is to merge the Gleaner.io GitHub repository into the first.","title":"Gleaner CLI Docker Image"},{"location":"cliDocker/#gleaner-cli-docker-image","text":"","title":"Gleaner CLI Docker Image"},{"location":"cliDocker/#about","text":"This is a new approach for quick starts with Gleaner. It is a script that exposes a containerized version of Gleaner as a CLI interface. You can use the -init flag to pull down all the support files you need including the Docker Compose file for setting up the object store, a triplestore and the support for headless indexing.","title":"About"},{"location":"cliDocker/#prerequisites","text":"You need Docker installed. Later, to work with the results and load them into a triplestore, you will also need an S3 compatible client. We will use the Minio client, mc, for this.","title":"Prerequisites"},{"location":"cliDocker/#steps","text":"Download the script gleanerDocker.sh from https://github.com/gleanerio/gleaner/tree/master/docs/cliDocker You may need to make it run-able with curl -O https://raw.githubusercontent.com/earthcubearchitecture-project418/gleaner/master/docs/cliDocker/gleanerDocker.sh chmod 755 gleanerDocker.sh Next you can run the script with the -init flag to pull down all the support files you need. ./gleanerDocker.sh -init This will also download the needed docker image and the support files. Your directory should look like this now: fils@ubuntu:~/clidocker# ls -lt total 1356 -rw-r--r-- 1 fils fils 1281 Aug 15 14:07 gleaner-IS.yml -rw-r--r-- 1 fils fils 290 Aug 15 14:07 setenvIS.sh -rw-r--r-- 1 fils fils 1266 Aug 15 14:07 demo.yaml -rw-r--r-- 1 fils fils 1371350 Aug 15 14:07 schemaorg-current-https.jsonld -rwxr-xr-x 1 fils fils 1852 Aug 15 14:06 gleanerDocker.sh Let's see if we can setup our support infrastructure for Gleaner. The file gleaner-IS.yml is a docker compose file that will set up the object store, and a triplestore. To do this we need to set up a few environment variables. To do this we can leverage the setenvIS.sh script. This script will set up the environment we need. Note you can also use a .env file or other approaches. You can references the Environment variables in Compose documentation. root@ubuntu:~/clidocker# source setenvIS.sh root@ubuntu:~/clidocker# docker-compose -f gleaner-IS.yml up -d Creating network \"clidocker_traefik_default\" with the default driver Creating clidocker_triplestore_1 ... done Creating clidocker_s3system_1 ... done Creating clidocker_headless_1 ... done Note: In a fresh run all the images will be pulled down. This may take a while. In the end, you should be able to see these images running: root@ubuntu:~/clidocker# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES a26f7c945479 nawer/blazegraph \"docker-entrypoint.s\u2026\" About a minute ago Up About a minute 0.0.0.0:9999->9999/tcp clidocker_triplestore_1 f3a4197c42be minio/minio:latest \"/usr/bin/docker-ent\u2026\" About a minute ago Up About a minute 0.0.0.0:9000->9000/tcp, 0.0.0.0:54321->54321/tcp clidocker_s3system_1 062f029462b1 chromedp/headless-shell:latest \"/headless-shell/hea\u2026\" About a minute ago Up About a minute 0.0.0.0:9222->9222/tcp At this point we should be able to do a run. During the init process a working config file was downloaded. Note: This config file will change... it's pointing to an OIH partner and I will not do that for the release. I have a demo site I will use. Next we need to setup our object for Gleaner. Gleaner itself can do this task so we will use root@ubuntu:~/clidocker# ./gleanerDocker.sh -setup -cfg demo main.go:35: EarthCube Gleaner main.go:110: Setting up buckets check.go:58: Gleaner Bucket gleaner not found, generating main.go:117: Buckets generated. Object store should be ready for runs Note: Here is where we go off the rails. The config file uses 0.0.0.0 as the location and this is not working. You need to edit the config file with the \"real\" IP of the host machine. In may case is this 192.168.122.77. This is obviously still a local network IP but it does work. I am still investigating this issue. We can now do a run with the example template file. Note: Best to delete the \"sitegraph\" node, I will do that soon. It should work, but is currently slow and gives little feedback If everything goes well, you should see something like the following: root@ubuntu:~/clidocker# ./gleanerDocker.sh -cfg demo main.go:35: EarthCube Gleaner main.go:122: Validating access to object store check.go:39: Validated access to object store: gleaner. org.go:156: Building organization graph (nq) org.go:163: {samplesearth https://samples.earth/sitemap.xml false https://www.re3data.org/repository/samplesearth Samples Earth (DEMO Site) https://samples.earth} main.go:154: Sitegraph(s) processed summoner.go:16: Summoner start time: 2021-08-15 14:34:08.907152656 +0000 UTC m=+0.067250623 resources.go:74: samplesearth : 202 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (202/202, 20 it/s) summoner.go:34: Summoner end time: 2021-08-15 14:34:20.36804137 +0000 UTC m=+11.528139340 summoner.go:35: Summoner run time: 0.191015 webfeed.go:37: 1758 millers.go:26: Miller start time: 2021-08-15 14:34:20.368063453 +0000 UTC m=+11.528161421 millers.go:40: Adding bucket to milling list: summoned/samplesearth millers.go:51: Adding bucket to prov building list: prov/samplesearth 100% |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| (202/202, 236 it/s) graphng.go:77: Assembling result graph for prefix: summoned/samplesearth to: milled/samplesearth graphng.go:78: Result graph will be at: results/runX/samplesearth_graph.nq pipecopy.go:16: Start pipe reader / writer sequence graphng.go:84: Pipe copy for graph done millers.go:80: Miller end time: 2021-08-15 14:34:21.84702814 +0000 UTC m=+13.007126109 millers.go:81: Miller run time: 0.024649","title":"Steps"},{"location":"cliDocker/#working-with-results","text":"If all has gone well, at this point you have downloaded the JSON-LD documents into Minio or some other object store.Next we will install a client that we can use to work with these objects. Note, there is a web interface exposed on the port mapped in the Docker compose file. In the case of these demo that is 9000. You can access it at http://localhost:9000/ with the credentials set in the environment variable file. However, to work with these objects it would be better to use a command line tool, like mc. The Minio Client, can be installed following their Minio Client Quickstate Guide . Be sure to place it somewhere where it can be seen from the command line, ie, make sure it is in your PATH variable. If you are on Linux this might look something like: wget https://dl.min.io/client/mc/release/linux-amd64/mc chmod +x mc ./mc --help There is also a Minio Client Docker image that you can use as well but it will be more difficult to use with the following scripts due to container isolation. To man an entry in the mc config use: mc alias set oih http://localhost:9000 worldsbestaccesskey worldsbestsecretkey We should now be able to list our object store. We have set it up using the alias oih . user@ubuntu:~/clidocker# mc ls oih [2021-08-15 14:31:20 UTC] 0B gleaner/ user@ubuntu:~/clidocker# mc ls oih/gleaner [2021-08-19 13:36:04 UTC] 0B milled/ [2021-08-19 13:36:04 UTC] 0B orgs/ [2021-08-19 13:36:04 UTC] 0B prov/ [2021-08-19 13:36:04 UTC] 0B results/ [2021-08-19 13:36:04 UTC] 0B summoned/ You can explore mc and see how to copy and work with the object store.","title":"Working with results"},{"location":"cliDocker/#loading-to-the-triplestore","text":"As part of our Docker compose file we also spun up a triplestore. Let's use that now. Now Download the minio2blaze.sh script. curl -O https://raw.githubusercontent.com/earthcubearchitecture-project418/gleaner/master/scripts/minio2blaze.sh chmod 755 minio2blaze.sh The content we need to load into the triplestore needs to be in RDF for Blazegraph. We also need to tell the triplestore how we have encoded that RDF. If look in the object store at mc ls oih/gleaner/milled [2021-08-19 13:26:52 UTC] 0B samplesearth/ We should see a bucket that is holding the RDF data converted from the JSON-LD. Let's use this in our test. We can pass this path to the minio2blaze.sh script. This script will go looking for the mc command we installed above, so be sure it is in a PATH location that script can see. ./minio2blaze.sh oih/gleaner/milled/samplesearth ... lots of results removed If all has gone well, we should have RDF in the triplestore. We started our triplestore as part of the docker-compose.yml file. You can visit the triplestore at http://localhost:9999/blazegraph/#splash Note, you may have to try other addresses other than localhost if your networking is a bit different with Docker. For me, I had to use a real local IP address for my network, you might also try 0.0.0.0 . Hopefully you will see something like the following. We loaded into the default kb namespace, so we should be good there. We can see that is listed as the active namespace at the Current Namespace: kb report. Let's try a simple SPARQL query. Click on the Query tab to get to the query user interfaced. We can use something like: select * where { ?s ?p ?o } LIMIT 10 A very simple SPARQL to give us the first 10 results from the triplestore. If all has gone well, we should see something like: You can explore more about SPARQL and the wide range of queries you can do with it at the W3C SPARQL 1.1 Query Language reference.","title":"Loading to the triplestore"},{"location":"cliDocker/#conclusion","text":"We have attempted here to give a quick introduction to the use of Gleaner in a Docker environment. This is a very simple example, but it should give you an idea of the approach used. This approach can then be combined with other approaches documented to establish a more production oriented implementation. Most of this documentation will be located at the Gleaner.io GitHub repository and Gleaner repository. Note: The plan is to merge the Gleaner.io GitHub repository into the first.","title":"Conclusion"},{"location":"profiling/","text":"Profiling \uf0c1 About \uf0c1 From time to time I do some profiling to see where the larger memory activity is. Code \uf0c1 In the code there is a profile section in the main that can be commented in/out. You will need to make sure the imports include the following. runtime/trace github.com/pkg/profile Commands \uf0c1 With the code uncommented (it could be hidden in a flag too) just run the code as normal. The profiles will be generated. go tool pprof -png mem.pprof > out.png","title":"Profiling"},{"location":"profiling/#profiling","text":"","title":"Profiling"},{"location":"profiling/#about","text":"From time to time I do some profiling to see where the larger memory activity is.","title":"About"},{"location":"profiling/#code","text":"In the code there is a profile section in the main that can be commented in/out. You will need to make sure the imports include the following. runtime/trace github.com/pkg/profile","title":"Code"},{"location":"profiling/#commands","text":"With the code uncommented (it could be hidden in a flag too) just run the code as normal. The profiles will be generated. go tool pprof -png mem.pprof > out.png","title":"Commands"},{"location":"prov/","text":"Nanopubs for PROV \uf0c1 About \uf0c1 Beginning to think about how Gleaner can represent the resources it harvests. One thought is a nanopub on each resource. How would I include something like: URL SHA256 provided @ID (if provided) Part of this grew out of the talk of \"Terrior\" in the ESIP Summer meeting. It made me wonder if I could develop a finger print for the resource such that it would make reconciling duplicated resources easier. Note: Review https://w3c-ccg.github.io/ld-proofs/ and an example implementation at https://github.com/digitalbazaar/jsonld-signatures While I like the idea that we should all have PIDs on all the things, I don't feel this is realistic. I think we have to accept that there will be a lot of duplication across the net. The goal will be to be able to know which ones are likely the same. If we provide a set of factors that are a \"finger print\" for a resource then we can likely present this back to the user. References \uf0c1 http://nanoweb.dei.unipd.it/ Indieweb specifications: https://indieweb.org/specifications","title":"Nanopubs for PROV"},{"location":"prov/#nanopubs-for-prov","text":"","title":"Nanopubs for PROV"},{"location":"prov/#about","text":"Beginning to think about how Gleaner can represent the resources it harvests. One thought is a nanopub on each resource. How would I include something like: URL SHA256 provided @ID (if provided) Part of this grew out of the talk of \"Terrior\" in the ESIP Summer meeting. It made me wonder if I could develop a finger print for the resource such that it would make reconciling duplicated resources easier. Note: Review https://w3c-ccg.github.io/ld-proofs/ and an example implementation at https://github.com/digitalbazaar/jsonld-signatures While I like the idea that we should all have PIDs on all the things, I don't feel this is realistic. I think we have to accept that there will be a lot of duplication across the net. The goal will be to be able to know which ones are likely the same. If we provide a set of factors that are a \"finger print\" for a resource then we can likely present this back to the user.","title":"About"},{"location":"prov/#references","text":"http://nanoweb.dei.unipd.it/ Indieweb specifications: https://indieweb.org/specifications","title":"References"}]}